<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Max Zimmer</title> <meta name="author" content="Max Zimmer"> <meta name="description" content="list of all my publications - you can filter by tag."> <meta name="keywords" content="Max Zimmer, mathematics, machine learning, deep learning, optimization, neural network pruning, sparsity, quantization, AI4Science, sustainability, TU Berlin, Zuse Institute Berlin"> <meta property="og:site_name" content="Max Zimmer"> <meta property="og:type" content="website"> <meta property="og:title" content="Max Zimmer | publications"> <meta property="og:url" content="https://maxzimmer.org/publications/"> <meta property="og:description" content="list of all my publications - you can filter by tag."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="publications"> <meta name="twitter:description" content="list of all my publications - you can filter by tag."> <meta name="twitter:site" content="@maxzimmerberlin"> <meta name="twitter:creator" content="@maxzimmerberlin"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://maxzimmer.org/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script defer data-domain="maxzimmer.org" src="https://plausible.iol.zib.de/js/script.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Max Zimmer</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks &amp; activities</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/ai_news/">ai news &amp; links</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <div class="post"> <div class="header-bar"> <h1>publications</h1> <h2>list of all my publications - you can filter by tag.</h2> </div> </div> <div class="publication-tags"> <a href="#" class="btn btn-sm active" role="button" data-tag="all">All</a> <a href="#efficiency" class="btn btn-sm" role="button" data-tag="efficiency">Efficiency</a> <a href="#sustainability" class="btn btn-sm" role="button" data-tag="sustainability">Sustainability</a> <a href="#ai4science" class="btn btn-sm" role="button" data-tag="ai4science">AI4Science</a> <a href="#ai4math" class="btn btn-sm" role="button" data-tag="ai4math">AI4Math</a> <a href="#computational-algebra" class="btn btn-sm" role="button" data-tag="computational-algebra">Computational Algebra</a> <a href="#optimization" class="btn btn-sm" role="button" data-tag="optimization">Optimization</a> <a href="#federated-learning" class="btn btn-sm" role="button" data-tag="federated-learning">Federated Learning</a> <a href="#xai" class="btn btn-sm" role="button" data-tag="xai">XAI</a> <a href="#nlp" class="btn btn-sm" role="button" data-tag="nlp">NLP</a> <a href="#psychology" class="btn btn-sm" role="button" data-tag="psychology">Psychology</a> <a href="#algebraic-geometry" class="btn btn-sm" role="button" data-tag="algebraic-geometry">Algebraic Geometry</a> </div> <div class="publications"> <ol class="bibliography"> <li> <div class="row entry" data-tags="ai4math, algebraic-geometry"> <div class="pub-thumb-col" data-tags="ai4math, algebraic-geometry"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/patchworked_curves.png" data-zoomable=""> </div></div> <div id="geiselmann2026patchworked" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2602.06888" class="no-highlight" rel="external nofollow noopener" target="_blank"> 121 Patchworked Curves of Degree Seven </a> </div> <div class="author"> Z. Geiselmann, <a href="https://page.math.tu-berlin.de/~joswig/" rel="external nofollow noopener" target="_blank">M. Joswig</a>, L. Kastner, <a href="https://iol.zib.de/team/konrad-mundinger.html" rel="external nofollow noopener" target="_blank">K. Mundinger</a>, <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Christoph Spiegel, Marcel Wack, Max Zimmer' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '0'); ">3 more authors</span> </div> <div class="periodical"> <abbr class="badge">Preprint</abbr><span class="venue-name"><em>arXiv preprint arXiv:2602.06888</em></span><span class="year"> 2026</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2602.06888" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We classify all 121 real schemes of smooth real plane algebraic curves of degree seven by constructing patchworks. We provide an explicit method for constructing polynomials realizing each real scheme. The work demonstrates that every real scheme at this degree level can be realized as a T-curve, resolving a previously open question from 1996.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">geiselmann2026patchworked</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{121 Patchworked Curves of Degree Seven}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Geiselmann, Zoe and Joswig, Michael and Kastner, Lars and Mundinger, Konrad and Pokutta, Sebastian and Spiegel, Christoph and Wack, Marcel and Zimmer, Max}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2602.06888}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="nlp, psychology"> <div class="pub-thumb-col" data-tags="nlp, psychology"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/semantic_geometry.png" data-zoomable=""> </div></div> <div id="schiekiera2026associations" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2602.00628" class="no-highlight" rel="external nofollow noopener" target="_blank"> From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs </a> </div> <div class="author"> <a href="https://schiekiera.github.io" rel="external nofollow noopener" target="_blank">L. Schiekiera</a>, <em>M. Zimmer</em>, <a href="http://christopheroux.de/" rel="external nofollow noopener" target="_blank">C. Roux</a>, <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a>, and <a href="https://sites.google.com/site/fritzgntr/home" rel="external nofollow noopener" target="_blank">F. Günther</a> </div> <div class="periodical"> <abbr class="badge">Preprint</abbr><span class="venue-name"><em>arXiv preprint arXiv:2602.00628</em></span><span class="year"> 2026</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2602.00628" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We investigate the extent to which an LLM’s hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms – similarity-based forced choice and free association – over a shared 5,000-word vocabulary, collecting 17.5M+ trials to build behavior-based similarity matrices. Using representational similarity analysis, we compare behavioral geometries to layerwise hidden-state similarity and benchmark against FastText, BERT, and cross-model consensus. We find that forced-choice behavior aligns substantially more with hidden-state geometry than free association. In a held-out-words regression, behavioral similarity (especially forced choice) predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus, indicating that behavior-only measurements retain recoverable information about internal semantic geometry. Finally, we discuss implications for the ability of behavioral tasks to uncover hidden cognitive states.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">schiekiera2026associations</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Schiekiera, Louis and Zimmer, Max and Roux, Christophe and Pokutta, Sebastian and G{\"u}nther, Fritz}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2602.00628}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="optimization"> <div class="pub-thumb-col" data-tags="optimization"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/frankwolfe_lower_bounds.png" data-zoomable=""> </div></div> <div id="halbey2026lower" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2602.04378" class="no-highlight" rel="external nofollow noopener" target="_blank"> Lower Bounds for Frank-Wolfe on Strongly Convex Sets </a> </div> <div class="author"> J. Halbey, D. Deza, <em>M. Zimmer</em>, <a href="http://christopheroux.de/" rel="external nofollow noopener" target="_blank">C. Roux</a>, B. Stellato, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sebastian Pokutta' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '0'); ">1 more author</span> </div> <div class="periodical"> <abbr class="badge">Preprint</abbr><span class="venue-name"><em>arXiv preprint arXiv:2602.04378</em></span><span class="year"> 2026</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2602.04378" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We present a constructive lower bound demonstrating that Frank-Wolfe optimization requires Ω(1/√ε) iterations when both the objective function and constraint set possess smoothness and strong convexity. This result confirms that existing convergence guarantees of O(1/√ε) are optimal. We focus on a representative problem: minimizing a strongly convex quadratic over a Euclidean unit ball with the optimizer positioned at the boundary. We introduce a novel computational method for constructing worst-case trajectories and provide an analytical proof establishing our theoretical bound.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">halbey2026lower</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Lower Bounds for Frank-Wolfe on Strongly Convex Sets}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Halbey, Jannis and Deza, Daniel and Zimmer, Max and Roux, Christophe and Stellato, Bartolomeo and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2602.04378}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency"> <div class="pub-thumb-col" data-tags="efficiency"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/recon.png" data-zoomable=""> </div></div> <div id="urbano2025recon" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2505.13289" class="no-highlight" rel="external nofollow noopener" target="_blank"> RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization </a> </div> <div class="author"> A. Urbano, <a href="https://davidwromero.xyz" rel="external nofollow noopener" target="_blank">D. W. Romero</a>, <em>M. Zimmer</em>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">ICLR26</abbr><span class="venue-name"> <em>The Fourteenth International Conference on Learning Representations</em> </span><span class="year"> 2026</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2505.13289" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Real-world data often exhibits unknown or approximate symmetries, yet existing equivariant networks must commit to a fixed transformation group prior to training, e.g., continuous SO(2) rotations. This mismatch degrades performance when the actual data symmetries differ from those in the transformation group. We introduce RECON, a framework to discover each input’s intrinsic symmetry distribution from unlabeled data. RECON leverages class-pose decompositions and applies a data-driven normalization to align arbitrary reference frames into a common natural pose, yielding directly comparable and interpretable symmetry descriptors. We demonstrate effective symmetry discovery on 2D image benchmarks and – for the first time – extend it to 3D transformation groups, paving the way towards more flexible equivariant modeling.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">urbano2025recon</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Urbano, Alonso and Romero, David W and Zimmer, Max and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Fourteenth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="computational-algebra, ai4math, ai4science"> <div class="pub-thumb-col" data-tags="computational-algebra, ai4math, ai4science"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/neural_sos.png" data-zoomable=""> </div></div> <div id="pelleriti2025neural" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2510.13444" class="no-highlight" rel="external nofollow noopener" target="_blank"> Neural Sum-of-Squares: Certifying the Nonnegativity of Polynomials with Transformers </a> </div> <div class="author"> N. Pelleriti, <a href="http://christophspiegel.berlin/" rel="external nofollow noopener" target="_blank">C. Spiegel</a>, <a href="https://shiweiliuiiiiiii.github.io" rel="external nofollow noopener" target="_blank">S. Liu</a>, <a href="https://damaru2.github.io" rel="external nofollow noopener" target="_blank">D. Martínez-Rubio</a>, <em>M. Zimmer</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sebastian Pokutta' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '0'); ">1 more author</span> </div> <div class="periodical"> <abbr class="badge">ICLR26</abbr><span class="venue-name"> <em>The Fourteenth International Conference on Learning Representations</em> </span><span class="year"> 2026</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2510.13444" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ZIB-IOL/Neural-Sum-of-Squares" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Certifying nonnegativity of polynomials is a well-known NP-hard problem with direct applications spanning non-convex optimization, control, robotics, and beyond. A sufficient condition for nonnegativity is the Sum of Squares (SOS) property, i.e., it can be written as a sum of squares of other polynomials. In practice, however, certifying the SOS criterion remains computationally expensive and often involves solving a Semidefinite Program (SDP), whose dimensionality grows quadratically in the size of the monomial basis of the SOS expression; hence, various methods to reduce the size of the monomial basis have been proposed. In this work, we introduce the first learning-augmented algorithm to certify the SOS criterion. To this end, we train a Transformer model that predicts an almost-minimal monomial basis for a given polynomial, thereby drastically reducing the size of the corresponding SDP. Our overall methodology comprises three key components: efficient training dataset generation of over 100 million SOS polynomials, design and training of the corresponding Transformer architecture, and a systematic fallback mechanism to ensure correct termination, which we analyze theoretically. We validate our approach on over 200 benchmark datasets, achieving speedups of over 100× compared to state-of-the-art solvers and enabling the solution of instances where competing approaches fail. Our findings provide novel insights towards transforming the practical scalability of SOS programming.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pelleriti2025neural</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural Sum-of-Squares: Certifying the Nonnegativity of Polynomials with Transformers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pelleriti, Nico and Spiegel, Christoph and Liu, Shiwei and Mart{\'i}nez-Rubio, David and Zimmer, Max and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Fourteenth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency"> <div class="pub-thumb-col" data-tags="efficiency"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/sparseswaps.png" data-zoomable=""> </div></div> <div id="zimmer2025sparseswaps" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2512.10922" class="no-highlight" rel="external nofollow noopener" target="_blank"> SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale </a> </div> <div class="author"> <em>M. Zimmer</em>, <a href="http://christopheroux.de/" rel="external nofollow noopener" target="_blank">C. Roux</a>, M. Wagner, D. Hendrych, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">Preprint</abbr><span class="venue-name"><em>arXiv preprint arXiv:2512.10922</em></span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2512.10922" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>The resource requirements of Neural Networks can be significantly reduced through pruning – the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zimmer2025sparseswaps</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zimmer, Max and Roux, Christophe and Wagner, Moritz and Hendrych, Deborah and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2512.10922}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="optimization"> <div class="pub-thumb-col" data-tags="optimization"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/bit_rounding.png" data-zoomable=""> </div></div> <div id="kuzinowicz2025objective" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2512.10507" class="no-highlight" rel="external nofollow noopener" target="_blank"> Objective Coefficient Rounding and Almost Symmetries in Binary Programs </a> </div> <div class="author"> D. Kuzinowicz, P. Lichocki, G. Mexi, M. E. Pfetsch, <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Max Zimmer' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '0'); ">1 more author</span> </div> <div class="periodical"> <abbr class="badge">Preprint</abbr><span class="venue-name"><em>arXiv preprint arXiv:2512.10507</em></span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2512.10507" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ZIB-IOL/BitRoundingAlmostSymmetries" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This article investigates the interplay of rounding objective coefficients in binary programs and almost symmetries. Empirically, reducing the number of significant bits through rounding often leads to instances that are easier to solve. One reason can be that the amount of symmetries increases, which enables solvers to be more effective when they are exploited. This can signify that the original instance contains ’almost symmetries’. Furthermore, solving the rounded problems provides approximations to the original objective values. We empirically investigate these relations on instances of the capacitated facility location problem, the knapsack problem and a diverse collection of additional instances, using the solvers SCIP and CP-SAT. For all investigated problem classes, we show empirically that this yields faster algorithms with guaranteed solution quality. The influence of symmetry depends on the instance type and solver.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kuzinowicz2025objective</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Objective Coefficient Rounding and Almost Symmetries in Binary Programs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kuzinowicz, Dominik and Lichocki, Paweł and Mexi, Gioni and Pfetsch, Marc E. and Pokutta, Sebastian and Zimmer, Max}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2512.10507}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency"> <div class="pub-thumb-col" data-tags="efficiency"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/llm_compression.png" data-zoomable=""> </div></div> <div id="wagner2025freelunchllmcompression" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2510.14444" class="no-highlight" rel="external nofollow noopener" target="_blank"> A Free Lunch in LLM Compression: Revisiting Retraining after Pruning </a> </div> <div class="author"> M. Wagner, <a href="http://christopheroux.de/" rel="external nofollow noopener" target="_blank">C. Roux</a>, <em>M. Zimmer</em>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">Preprint</abbr><span class="venue-name"><em>arXiv preprint arXiv:2510.14444</em></span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2510.14444" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>While Neural Network pruning typically requires retraining the model to recover pruning-induced performance degradation, state-of-the-art Large Language Models (LLMs) pruning methods instead solve a layer-wise mask selection and reconstruction problem on a small set of calibration data to avoid full retraining, as it is considered computationally infeasible for LLMs. Reconstructing single matrices in isolation has favorable properties, such as convexity of the objective and significantly reduced memory requirements compared to full retraining. In practice, however, reconstruction is often implemented at coarser granularities, e.g., reconstructing a whole transformer block against its dense activations instead of a single matrix. In this work, we study the key design choices when reconstructing or retraining the remaining weights after pruning. We conduct an extensive computational study on state-of-the-art GPT architectures, and report several surprising findings that challenge common intuitions about retraining after pruning. In particular, we observe a free lunch scenario: reconstructing attention and MLP components separately within each transformer block is nearly the most resource-efficient yet achieves the best perplexity. Most importantly, this Pareto-optimal setup achieves better performance than full retraining, despite requiring only a fraction of the memory. Furthermore, we demonstrate that simple and efficient pruning criteria such as Wanda can outperform much more complex approaches when the reconstruction step is properly executed, highlighting its importance. Our findings challenge the narrative that retraining should be avoided at all costs and provide important insights into post-pruning performance recovery for LLMs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wagner2025freelunchllmcompression</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Free Lunch in LLM Compression: Revisiting Retraining after Pruning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wagner, Moritz and Roux, Christophe and Zimmer, Max and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2510.14444}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency, optimization"> <div class="pub-thumb-col" data-tags="efficiency, optimization"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/frankwolfe_pruning.png" data-zoomable=""> </div></div> <div id="roux2025dontbegreedyjustrelax" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2510.13713" class="no-highlight" rel="external nofollow noopener" target="_blank"> Don’t Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe </a> </div> <div class="author"> <a href="http://christopheroux.de/" rel="external nofollow noopener" target="_blank">C. Roux</a>, <em>M. Zimmer</em>, A. d’Aspremont, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">Preprint</abbr><span class="venue-name"><em>arXiv preprint arXiv:2510.13713</em></span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2510.13713" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Pruning is a common technique to reduce the compute and storage requirements of Neural Networks. While conventional approaches typically retrain the model to recover pruning-induced performance degradation, state-of-the-art Large Language Model (LLM) pruning methods operate layer-wise, minimizing the per-layer pruning error on a small calibration dataset to avoid full retraining, which is considered computationally prohibitive for LLMs. However, finding the optimal pruning mask is a hard combinatorial problem and solving it to optimality is intractable. Existing methods hence rely on greedy heuristics that ignore the weight interactions in the pruning objective. In this work, we instead consider the convex relaxation of these combinatorial constraints and solve the resulting problem using the Frank-Wolfe (FW) algorithm. Our method drastically reduces the per-layer pruning error, outperforms strong baselines on state-of-the-art GPT architectures, and remains memory-efficient. We provide theoretical justification by showing that, combined with the convergence guarantees of the FW algorithm, we obtain an approximate solution to the original combinatorial problem upon rounding the relaxed solution to integrality.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">roux2025dontbegreedyjustrelax</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Roux, Christophe and Zimmer, Max and d'Aspremont, Alexandre and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2510.13713}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="ai4science, computational-algebra, ai4math"> <div class="pub-thumb-col" data-tags="ai4science, computational-algebra, ai4math"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/obba.png" data-zoomable=""> </div></div> <div id="kera2025computationalalgebraattentiontransformer" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2505.23696" class="no-highlight" rel="external nofollow noopener" target="_blank"> Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms </a> </div> <div class="author"> H. Kera, N. Pelleriti, Y. Ishihara, <em>M. Zimmer</em>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">NEURIPS25</abbr><span class="venue-name"> <em>Advances in Neural Information Processing Systems</em> </span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2505.23696" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/HiroshiKERA/OracleBorderBasis" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/OBBA-poster-NeurIPS.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Solving systems of polynomial equations, particularly those with finitely many solutions, is a crucial challenge across many scientific fields. Traditional methods like Gröbner and Border bases are fundamental but suffer from high computational costs, which have motivated recent Deep Learning approaches to improve efficiency, albeit at the expense of output correctness. In this work, we introduce the Oracle Border Basis Algorithm, the first Deep Learning approach that accelerates Border basis computation while maintaining output guarantees. To this end, we design and train a Transformer-based oracle that identifies and eliminates computationally expensive reduction steps, which we find to dominate the algorithm’s runtime. By selectively invoking this oracle during critical phases of computation, we achieve substantial speedup factors of up to 3.5x compared to the base algorithm, without compromising the correctness of results. To generate the training data, we develop a sampling method and provide the first sampling theorem for border bases. We construct a tokenization and embedding scheme tailored to monomial-centered algebraic computations, resulting in a compact and expressive input representation, which reduces the number of tokens to encode an n-variate polynomial by a factor of O(n). Our learning approach is data efficient, stable, and a practical enhancement to traditional computer algebra algorithms and symbolic computation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kera2025computationalalgebraattentiontransformer</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kera, Hiroshi and Pelleriti, Nico and Ishihara, Yuki and Zimmer, Max and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{38}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="ai4science, ai4math"> <div class="pub-thumb-col" data-tags="ai4science, ai4math"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/colordreaming.png" data-zoomable=""> </div></div> <div id="mundinger2025neural" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2501.18527" class="no-highlight" rel="external nofollow noopener" target="_blank"> Neural Discovery in Mathematics: Do Machines Dream of Colored Planes? </a> </div> <div class="author"> <a href="https://iol.zib.de/team/konrad-mundinger.html" rel="external nofollow noopener" target="_blank">K. Mundinger</a>, <em>M. Zimmer</em>, A. Kiem, <a href="http://christophspiegel.berlin/" rel="external nofollow noopener" target="_blank">C. Spiegel</a>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">ICML25</abbr><span class="venue-name"> <em>Forty-second International Conference on Machine Learning (Oral presentation, top 1%)</em> </span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2501.18527" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="blog/2025/neural-discovery-in-mathematics-do-machines-dream-of-colored-planes" class="btn btn-sm" role="button">Blog</a> <a href="https://github.com/ZIB-IOL/neural-discovery-icml25" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/NeuralDiscovery-poster-ICML.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We demonstrate how neural networks can drive mathematical discovery through a case study of the Hadwiger-Nelson problem, a long-standing open problem from discrete geometry and combinatorics about coloring the plane avoiding monochromatic unit-distance pairs. Using neural networks as approximators, we reformulate this mixed discrete-continuous geometric coloring problem as an optimization task with a probabilistic, differentiable loss function. This enables gradient-based exploration of admissible configurations that most significantly led to the discovery of two novel six-colorings, providing the first improvements in thirty years to the off-diagonal variant of the original problem (Mundinger et al., 2024a). Here, we establish the underlying machine learning approach used to obtain these results and demonstrate its broader applicability through additional results and numerical insights.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mundinger2025neural</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural Discovery in Mathematics: Do Machines Dream of Colored Planes?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mundinger, Konrad and Zimmer, Max and Kiem, Aldo and Spiegel, Christoph and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Forty-second International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=7Tp9zjP9At}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="sustainability, ai4science"> <div class="pub-thumb-col" data-tags="sustainability, ai4science"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/dunia.png" data-zoomable=""> </div></div> <div id="fayad2025dunia" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2502.17066" class="no-highlight" rel="external nofollow noopener" target="_blank"> DUNIA: Pixel-Sized Embeddings via Cross-Modal Alignment for Earth Observation Applications </a> </div> <div class="author"> I. Fayad, <em>M. Zimmer</em>, M. Schwartz, <a href="https://www.lsce.ipsl.fr/Phocea/Pisp/index.php?nom=philippe.ciais" rel="external nofollow noopener" target="_blank">P. Ciais</a>, <a href="https://www.wi.uni-muenster.de/department/dasc/people/fabian-gieseke" rel="external nofollow noopener" target="_blank">F. Gieseke</a>, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Gabriel Belouze, Sarah Brood, Aurelien De Truchis, Alexandre d’Aspremont' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '0'); ">4 more authors</span> </div> <div class="periodical"> <abbr class="badge">ICML25</abbr><span class="venue-name"> <em>Forty-second International Conference on Machine Learning</em> </span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2502.17066" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/AI4Forest/DUNIA" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/DUNIA-poster-ICML.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Significant efforts have been directed towards adapting self-supervised multimodal learning for Earth observation applications. However, existing methods produce coarse patch-sized embeddings, limiting their effectiveness and integration with other modalities like LiDAR. To close this gap, we present DUNIA, an approach to learn pixel-sized embeddings through cross-modal alignment between images and full-waveform LiDAR data. As the model is trained in a contrastive manner, the embeddings can be directly leveraged in the context of a variety of environmental monitoring tasks in a zero-shot setting. In our experiments, we demonstrate the effectiveness of the embeddings for seven such tasks (canopy height mapping, fractional canopy cover, land cover mapping, tree species identification, plant area index, crop type classification, and per-pixel waveform-based vertical structure mapping). The results show that the embeddings, along with zero-shot classifiers, often outperform specialized supervised models, even in low data regimes. In the fine-tuning setting, we show strong low-shot capabilities with performances near or better than state-of-the-art on five out of six tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">fayad2025dunia</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DUNIA: Pixel-Sized Embeddings via Cross-Modal Alignment for Earth Observation Applications}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fayad, Ibrahim and Zimmer, Max and Schwartz, Martin and Ciais, Philippe and Gieseke, Fabian and Belouze, Gabriel and Brood, Sarah and De Truchis, Aurelien and d'Aspremont, Alexandre}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Forty-second International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=JXCiQteuOv}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency, computational-algebra"> <div class="pub-thumb-col" data-tags="efficiency, computational-algebra"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/vanishingideals.png" data-zoomable=""> </div></div> <div id="pelleriti2025approximatinglatentmanifoldsneural" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2502.15051" class="no-highlight" rel="external nofollow noopener" target="_blank"> Approximating Latent Manifolds in Neural Networks via Vanishing Ideals </a> </div> <div class="author"> N. Pelleriti, <em>M. Zimmer</em>, <a href="https://elwirth.github.io" rel="external nofollow noopener" target="_blank">E. Wirth</a>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">ICML25</abbr><span class="venue-name"> <em>Forty-second International Conference on Machine Learning</em> </span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2502.15051" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="blog/2025/approximating-latent-manifolds-in-neural-networks" class="btn btn-sm" role="button">Blog</a> <a href="https://github.com/ZIB-IOL/approximating-neural-network-manifolds" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/ApproximatingLatentManifolds-poster-ICML.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Deep neural networks have reshaped modern machine learning by learning powerful latent representations that often align with the manifold hypothesis: high-dimensional data lie on lower-dimensional manifolds. In this paper, we establish a connection between manifold learning and computational algebra by demonstrating how vanishing ideals can characterize the latent manifolds of deep networks. To that end, we propose a new neural architecture that (i) truncates a pretrained network at an intermediate layer, (ii) approximates each class manifold via polynomial generators of the vanishing ideal, and (iii) transforms the resulting latent space into linearly separable features through a single polynomial layer. The resulting models have significantly fewer layers than their pretrained baselines, while maintaining comparable accuracy, achieving higher throughput, and utilizing fewer parameters. Furthermore, drawing on spectral complexity analysis, we derive sharper theoretical guarantees for generalization, showing that our approach can in principle offer tighter bounds than standard deep networks. Numerical experiments confirm the effectiveness and efficiency of the proposed approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pelleriti2025approximatinglatentmanifoldsneural</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Approximating Latent Manifolds in Neural Networks via Vanishing Ideals}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pelleriti, Nico and Zimmer, Max and Wirth, Elias and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Forty-second International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=WYlerYGDPL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="sustainability, ai4science"> <div class="pub-thumb-col" data-tags="sustainability, ai4science"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/temporalcanopy.png" data-zoomable=""> </div></div> <div id="pauls2025capturing" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2501.19328" class="no-highlight" rel="external nofollow noopener" target="_blank"> Capturing Temporal Dynamics in Large-Scale Canopy Tree Height Estimation </a> </div> <div class="author"> <a href="https://janpauls.org" rel="external nofollow noopener" target="_blank">J. Pauls</a>, <em>M. Zimmer</em>, <a href="https://b-turan.github.io" rel="external nofollow noopener" target="_blank">B. Turan</a>, <a href="https://science.jpl.nasa.gov/people/saatchi/" rel="external nofollow noopener" target="_blank">S. Saatchi</a>, <a href="https://www.lsce.ipsl.fr/Phocea/Pisp/index.php?nom=philippe.ciais" rel="external nofollow noopener" target="_blank">P. Ciais</a>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Sebastian Pokutta, Fabian Gieseke' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '0'); ">2 more authors</span> </div> <div class="periodical"> <abbr class="badge">ICML25</abbr><span class="venue-name"> <em>Forty-second International Conference on Machine Learning</em> </span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2501.19328" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="blog/2025/capturing-temporal-dynamics-in-tree-canopy-height" class="btn btn-sm" role="button">Blog</a> <a href="https://github.com/AI4Forest/Europe-Temporal-Canopy-Height" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/TemporalCanopy-poster-ICML.pdf" class="btn btn-sm" role="button">Poster</a> <a href="https://europetreemap.projects.earthengine.app/view/temporalcanopyheight" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Earth Engine</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>With the rise in global greenhouse gas emissions, accurate large-scale tree canopy height maps are essential for understanding forest structure, estimating above-ground biomass, and monitoring ecological disruptions. To this end, we present a novel approach to generate large-scale, high-resolution canopy height maps over time. Our model accurately predicts canopy height over multiple years given Sentinel-2 time series satellite data. Using GEDI LiDAR data as the ground truth for training the model, we present the first 10m resolution temporal canopy height map of the European continent for the period 2019-2022. As part of this product, we also offer a detailed canopy height map for 2020, providing more precise estimates than previous studies. Our pipeline and the resulting temporal height map are publicly available, enabling comprehensive large-scale monitoring of forests and, hence, facilitating future research and ecological analyses. For an interactive viewer, see this https URL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pauls2025capturing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Capturing Temporal Dynamics in Large-Scale Canopy Tree Height Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pauls, Jan and Zimmer, Max and Turan, Berkant and Saatchi, Sassan and Ciais, Philippe and Pokutta, Sebastian and Gieseke, Fabian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Forty-second International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=ri1cs3vtXX}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency, optimization"> <div class="pub-thumb-col" data-tags="efficiency, optimization"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/sparsefw_pruneddistance.png" data-zoomable=""> </div></div> <div id="ZimmerSpiegelPokutta+2025+137+168" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2205.11921" class="no-highlight" rel="external nofollow noopener" target="_blank"> Compression-aware Training of Neural Networks using Frank-Wolfe </a> </div> <div class="author"> <em>M. Zimmer</em>, <a href="http://christophspiegel.berlin/" rel="external nofollow noopener" target="_blank">C. Spiegel</a>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">Journal</abbr><span class="venue-name"> <em>Mathematical Optimization for Machine Learning</em> </span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2205.11921" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ZIB-IOL/compression-aware-SFW" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Many existing Neural Network pruning approaches either rely on retraining to compensate for pruning-caused performance degradation or they induce strong biases to converge to a specific sparse solution throughout training. A third paradigm obtains a wide range of compression ratios from a single dense training run while also avoiding retraining. Recent work of Pokutta et al. (2020) and Miao et al. (2022) suggests that the Stochastic Frank-Wolfe (SFW) algorithm is particularly suited for training state-of-the-art models that are robust to compression. We propose leveraging k-support norm ball constraints and demonstrate significant improvements over the results of Miao et al. (2022) in the case of unstructured pruning. We also extend these ideas to the structured pruning domain and propose novel approaches to both ensure robustness to the pruning of convolutional filters as well as to low-rank tensor decompositions of convolutional layers. In the latter case, our approach performs on-par with nuclear-norm regularization baselines while requiring only half of the computational resources. Our findings also indicate that the robustness of SFW-trained models largely depends on the gradient rescaling of the learning rate and we establish a theoretical foundation for that practice.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inbook</span><span class="p">{</span><span class="nl">ZimmerSpiegelPokutta+2025+137+168</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1515/9783111376776-010}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Compression-aware Training of Neural Networks using Frank-Wolfe}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Mathematical Optimization for Machine Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zimmer, Max and Spiegel, Christoph and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Fackeldey, Konstantin and Kannan, Aswin and Pokutta, Sebastian and Sharma, Kartikey and Walter, Daniel and Walther, Andrea and Weiser, Martin}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{De Gruyter}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Berlin, Boston}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{137--168}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{doi:10.1515/9783111376776-010}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9783111376776}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency"> <div class="pub-thumb-col" data-tags="efficiency"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/cs256.png" data-zoomable=""> </div></div> <div id="lasby2025compressed" class="pub-content-col"> <div class="title"> <a href="https://openreview.net/forum?id=iso0KV2HVq" class="no-highlight" rel="external nofollow noopener" target="_blank"> Compressed Sparse Tiles for Memory-Efficient Unstructured and Semi-Structured Sparsity </a> </div> <div class="author"> M. Lasby, <em>M. Zimmer</em>, <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a>, and E. Schultheis</div> <div class="periodical"> <abbr class="badge">Workshop</abbr><span class="venue-name"> <em>ICLR25 Workshop on Sparsity in LLMs (SLLM)</em> </span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://openreview.net/forum?id=iso0KV2HVq" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/CS256-poster-ICLR.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Storing the weights of large language models (LLMs) in GPU memory for local inference is challenging due to their size. While quantization has proven successful in reducing the memory footprint of LLMs, unstructured pruning introduces overhead by requiring the non-pruned weights’ location to be encoded. This overhead hinders the efficient combination of quantization and unstructured pruning, especially for smaller batch sizes common in inference scenarios. To address this, we propose the CS256 storage format, which offers a better balance between space efficiency and hardware acceleration compared to existing formats. CS256 partitions the weight matrix into tiles and uses a hierarchical indexing scheme to locate non-zero values, reducing the overhead associated with storing sparsity patterns. Our preliminary results with one-shot pruning of LLMs show that CS256 matches the performance of unstructured sparsity while being more hardware-friendly.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lasby2025compressed</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Compressed Sparse Tiles for Memory-Efficient Unstructured and Semi-Structured Sparsity}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lasby, Mike and Zimmer, Max and Pokutta, Sebastian and Schultheis, Erik}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Sparsity in LLMs (SLLM): Deep Dive into Mixture of Experts, Quantization, Hardware, and Inference}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=iso0KV2HVq}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="federated-learning"> <div class="pub-thumb-col" data-tags="federated-learning"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/byzantine_feddistill.png" data-zoomable=""> </div></div> <div id="roux2024byzantine" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2402.12265" class="no-highlight" rel="external nofollow noopener" target="_blank"> On the Byzantine-Resilience of Distillation-Based Federated Learning </a> </div> <div class="author"> <a href="http://christopheroux.de/" rel="external nofollow noopener" target="_blank">C. Roux</a>, <em>M. Zimmer</em>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">ICLR25</abbr><span class="venue-name"> <em>The Thirteenth International Conference on Learning Representations</em> </span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2402.12265" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.pokutta.com/blog/research/2024/11/19/federated-learning.html" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/ZIB-IOL/FedDistill" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/FedDistill-poster-ICLR.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset. In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process. We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to Federated Averaging. Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods. Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilience of KD-based FL algorithms and demonstrate its efficacy. Finally, we provide a general method to make attacks harder to detect, improving their effectiveness.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">roux2024byzantine</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Roux, Christophe and Zimmer, Max and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Byzantine-Resilience of Distillation-Based Federated Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Thirteenth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=of6EuHT7de}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="sustainability, ai4science"> <div class="pub-thumb-col" data-tags="sustainability, ai4science"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/canopy_height.png" data-zoomable=""> </div></div> <div id="pauls2024estimating" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2406.01076" class="no-highlight" rel="external nofollow noopener" target="_blank"> Estimating Canopy Height at Scale </a> </div> <div class="author"> <a href="https://janpauls.org" rel="external nofollow noopener" target="_blank">J. Pauls</a>, <em>M. Zimmer</em>, <a href="https://www.wi.uni-muenster.de/de/institut/dasc/personen/una-kelly" rel="external nofollow noopener" target="_blank">U. M. Kelly</a>, M. Schwartz, <a href="https://science.jpl.nasa.gov/people/saatchi/" rel="external nofollow noopener" target="_blank">S. Saatchi</a>, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Philippe Ciais, Sebastian Pokutta, Martin Brandt, Fabian Gieseke' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '0'); ">4 more authors</span> </div> <div class="periodical"> <abbr class="badge">ICML24</abbr><span class="venue-name"> <em>Forty-first International Conference on Machine Learning</em> </span><span class="year"> 2024</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2406.01076" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="blog/2025/estimating-canopy-height-at-scale/" class="btn btn-sm" role="button">Blog</a> <a href="https://github.com/AI4Forest/Global-Canopy-Height-Map" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/Canopy-poster-ICML.pdf" class="btn btn-sm" role="button">Poster</a> <a href="https://worldwidemap.projects.earthengine.app/view/canopy-height-2020" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Earth Engine</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We propose a framework for global-scale canopy height estimation based on satellite data. Our model leverages advanced data preprocessing techniques, resorts to a novel loss function designed to counter geolocation inaccuracies inherent in the ground-truth height measurements, and employs data from the Shuttle Radar Topography Mission to effectively filter out erroneous labels in mountainous regions, enhancing the reliability of our predictions in those areas. A comparison between predictions and ground-truth labels yields an MAE / RMSE of 2.43 / 4.73 (meters) overall and 4.45 / 6.72 (meters) for trees taller than five meters, which depicts a substantial improvement compared to existing global-scale maps. The resulting height map as well as the underlying framework will facilitate and enhance ecological analyses at a global scale, including, but not limited to, large-scale forest and biomass monitoring.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pauls2024estimating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Estimating Canopy Height at Scale}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pauls, Jan and Zimmer, Max and Kelly, Una M. and Schwartz, Martin and Saatchi, Sassan and Ciais, Philippe and Pokutta, Sebastian and Brandt, Martin and Gieseke, Fabian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Forty-first International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=ZzCY0fRver}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="ai4science, ai4math"> <div class="pub-thumb-col" data-tags="ai4science, ai4math"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/hadwigernelson.png" data-zoomable=""> </div></div> <div id="mundinger2024extending" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2404.05509" class="no-highlight" rel="external nofollow noopener" target="_blank"> Extending the Continuum of Six-Colorings </a> </div> <div class="author"> <a href="https://iol.zib.de/team/konrad-mundinger.html" rel="external nofollow noopener" target="_blank">K. Mundinger</a>, <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a>, <a href="http://christophspiegel.berlin/" rel="external nofollow noopener" target="_blank">C. Spiegel</a>, and <em>M. Zimmer</em> </div> <div class="periodical"> <abbr class="badge">Journal</abbr><span class="venue-name"><em>Geombinatorics Quarterly</em></span><span class="year"> 2024</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2404.05509" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.pokutta.com/blog/research/2024/07/28/hadwiger-nelson.html" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Blog</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We present two novel six-colorings of the Euclidean plane that avoid monochromatic pairs of points at unit distance in five colors and monochromatic pairs at another specified distance d in the sixth color. Such colorings have previously been known to exist for 0.41 &lt; \sqrt2 - 1 \le d \le 1 / \sqrt5 &lt; 0.45. Our results significantly expand that range to 0.354 \le d \le 0.657, the first improvement in 30 years. Notably, the constructions underlying this were derived by formalizing colorings suggested by a custom machine learning approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mundinger2024extending</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mundinger, Konrad and Pokutta, Sebastian and Spiegel, Christoph and Zimmer, Max}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Geombinatorics Quarterly}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Extending the Continuum of Six-Colorings}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{XXXIV}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2404.05509}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://geombina.uccs.edu/past-issues/volume-xxxiv}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="ai4science, ai4math"> <div class="pub-thumb-col" data-tags="ai4science, ai4math"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/pde.png" data-zoomable=""> </div></div> <div id="mundinger2024neural" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2403.12764" class="no-highlight" rel="external nofollow noopener" target="_blank"> Neural Parameter Regression for Explicit Representations of PDE Solution Operators </a> </div> <div class="author"> <a href="https://iol.zib.de/team/konrad-mundinger.html" rel="external nofollow noopener" target="_blank">K. Mundinger</a>, <em>M. Zimmer</em>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">Workshop</abbr><span class="venue-name"> <em>ICLR24 Workshop on AI4DifferentialEquations In Science</em> </span><span class="year"> 2024</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2403.12764" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/NPR-poster-ICLR.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We introduce Neural Parameter Regression (NPR), a novel framework specifically developed for learning solution operators in Partial Differential Equations (PDEs). Tailored for operator learning, this approach surpasses traditional DeepONets (Lu et al., 2021) by employing Physics-Informed Neural Network (PINN, Raissi et al., 2019) techniques to regress Neural Network (NN) parameters. By parametrizing each solution based on specific initial conditions, it effectively approximates a mapping between function spaces. Our method enhances parameter efficiency by incorporating low-rank matrices, thereby boosting computational efficiency and scalability. The framework shows remarkable adaptability to new initial and boundary conditions, allowing for rapid fine-tuning and inference, even in cases of out-of-distribution examples.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mundinger2024neural</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mundinger, Konrad and Zimmer, Max and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural Parameter Regression for Explicit Representations of PDE Solution Operators}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICLR 2024 Workshop on AI4DifferentialEquations In Science}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=6Z0q0dzSJQ}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency"> <div class="pub-thumb-col" data-tags="efficiency"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/sms_algo_sketch.png" data-zoomable=""> </div></div> <div id="zimmer2023sparse" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2306.16788" class="no-highlight" rel="external nofollow noopener" target="_blank"> Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging </a> </div> <div class="author"> <em>M. Zimmer</em>, <a href="http://christophspiegel.berlin/" rel="external nofollow noopener" target="_blank">C. Spiegel</a>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">ICLR24</abbr><span class="venue-name"> <em>The Twelfth International Conference on Learning Representations</em> </span><span class="year"> 2024</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2306.16788" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.pokutta.com/blog/research/2023/08/05/abstract-SMS.html" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/ZIB-IOL/SMS" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/SMS-poster-ICLR.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Neural networks can be significantly compressed by pruning, leading to sparse models requiring considerably less storage and floating-point operations while maintaining predictive performance. Model soups (Wortsman et al., 2022) improve generalization and out-of-distribution performance by averaging the parameters of multiple models into a single one without increased inference time. However, identifying models in the same loss basin to leverage both sparsity and parameter averaging is challenging, as averaging arbitrary sparse models reduces the overall sparsity due to differing sparse connectivities. In this work, we address these challenges by demonstrating that exploring a single retraining phase of Iterative Magnitude Pruning (IMP) with varying hyperparameter configurations, such as batch ordering or weight decay, produces models that are suitable for averaging and share the same sparse connectivity by design. Averaging these models significantly enhances generalization performance compared to their individual components. Building on this idea, we introduce Sparse Model Soups (SMS), a novel method for merging sparse models by initiating each prune-retrain cycle with the averaged model of the previous phase. SMS maintains sparsity, exploits sparse network benefits being modular and fully parallelizable, and substantially improves IMP’s performance. Additionally, we demonstrate that SMS can be adapted to enhance the performance of state-of-the-art pruning during training approaches.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zimmer2023sparse</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zimmer, Max and Spiegel, Christoph and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="xai"> <div class="pub-thumb-col" data-tags="xai"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/arthurmerlin.png" data-zoomable=""> </div></div> <div id="waldchen2024interpretability" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2206.00759" class="no-highlight" rel="external nofollow noopener" target="_blank"> Interpretability Guarantees with Merlin-Arthur Classifiers </a> </div> <div class="author"> <a href="https://stephanw.net/" rel="external nofollow noopener" target="_blank">S. Wäldchen</a>, <a href="https://kartikeyrinwa.github.io/" rel="external nofollow noopener" target="_blank">K. Sharma</a>, <a href="https://b-turan.github.io" rel="external nofollow noopener" target="_blank">B. Turan</a>, <em>M. Zimmer</em>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge"><a href="https://virtual.aistats.org/Conferences/2024" rel="external nofollow noopener" target="_blank">AISTATS24</a></abbr><span class="venue-name"> <em>International Conference on Artificial Intelligence and Statistics</em> </span><span class="year"> 2024</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2206.00759" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/AM-poster-AISTATS.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We propose an interactive multi-agent classifier that provides provable interpretability guarantees even for complex agents such as neural networks. These guarantees consist of lower bounds on the mutual information between selected features and the classification decision. Our results are inspired by the Merlin-Arthur protocol from Interactive Proof Systems and express these bounds in terms of measurable metrics such as soundness and completeness. Compared to existing interactive setups, we rely neither on optimal agents nor on the assumption that features are distributed independently. Instead, we use the relative strength of the agents as well as the new concept of Asymmetric Feature Correlation which captures the precise kind of correlations that make interpretability guarantees difficult. We evaluate our results on two small-scale datasets where high mutual information can be verified explicitly. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">waldchen2024interpretability</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Interpretability Guarantees with Merlin-Arthur Classifiers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{W{\"a}ldchen, Stephan and Sharma, Kartikey and Turan, Berkant and Zimmer, Max and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Artificial Intelligence and Statistics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency"> <div class="pub-thumb-col" data-tags="efficiency"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/perp_dog.png" data-zoomable=""> </div></div> <div id="zimmer2023perp" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2312.15230" class="no-highlight" rel="external nofollow noopener" target="_blank"> PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs </a> </div> <div class="author"> <em>M. Zimmer</em>, M. Andoni, <a href="http://christophspiegel.berlin/" rel="external nofollow noopener" target="_blank">C. Spiegel</a>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">Preprint</abbr><span class="venue-name"><em>arXiv preprint arXiv:2312.15230</em></span><span class="year"> 2023</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2312.15230" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ZIB-IOL/PERP" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Neural Networks can be effectively compressed through pruning, significantly reducing storage and compute demands while maintaining predictive performance. Simple yet effective methods like magnitude pruning remove less important parameters and typically require a costly retraining procedure to restore performance. However, with the rise of LLMs, full retraining has become infeasible due to memory and compute constraints. This study challenges the practice of retraining all parameters by showing that updating a small subset of highly expressive parameters can suffice to recover or even enhance performance after pruning. Surprisingly, retraining just 0.01%-0.05% of the parameters in GPT-architectures can match the performance of full retraining across various sparsity levels, significantly reducing compute and memory requirements, and enabling retraining of models with up to 30 billion parameters on a single GPU in minutes. To bridge the gap to full retraining in the high sparsity regime, we introduce two novel LoRA variants that, unlike standard LoRA, allow merging adapters back without compromising sparsity. Going a step further, we show that these methods can be applied for memory-efficient layer-wise reconstruction, significantly enhancing state-of-the-art retraining-free methods like Wanda (Sun et al., 2023) and SparseGPT (Frantar &amp; Alistarh, 2023). Our findings present a promising alternative to avoiding retraining.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zimmer2023perp</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zimmer, Max and Andoni, Megi and Spiegel, Christoph and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2312.15230}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency"> <div class="pub-thumb-col" data-tags="efficiency"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/bimp_lr_schedules.png" data-zoomable=""> </div></div> <div id="Zimmer2023" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2111.00843" class="no-highlight" rel="external nofollow noopener" target="_blank"> How I Learned To Stop Worrying And Love Retraining </a> </div> <div class="author"> <em>M. Zimmer</em>, <a href="http://christophspiegel.berlin/" rel="external nofollow noopener" target="_blank">C. Spiegel</a>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">ICLR23</abbr><span class="venue-name"> <em>The Eleventh International Conference on Learning Representations</em> </span><span class="year"> 2023</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2111.00843" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.pokutta.com/blog/research/2023/07/21/abstract-BIMP.html" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/ZIB-IOL/BIMP" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/BIMP-poster-ICLR.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Many Neural Network Pruning approaches consist of several iterative training and pruning steps, seemingly losing a significant amount of their performance after pruning and then recovering it in the subsequent retraining phase. Recent works of Renda et al. (2020) and Le &amp; Hua (2021) demonstrate the significance of the learning rate schedule during the retraining phase and propose specific heuristics for choosing such a schedule for IMP (Han et al., 2015). We place these findings in the context of the results of Li et al. (2020) regarding the training of models within a fixed training budget and demonstrate that, consequently, the retraining phase can be massively shortened using a simple linear learning rate schedule. Improving on existing retraining approaches, we additionally propose a method to adaptively select the initial value of the linear schedule. Going a step further, we propose similarly imposing a budget on the initial dense training phase and show that the resulting simple and efficient method is capable of outperforming significantly more complex or heavily parameterized state-of-the-art approaches that attempt to sparsify the network during training. These findings not only advance our understanding of the retraining phase, but more broadly question the belief that one should aim to avoid the need for retraining and reduce the negative effects of ‘hard’ pruning by incorporating the sparsification process into the standard training.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Zimmer2023</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zimmer, Max and Spiegel, Christoph and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Eleventh International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{H}ow {I} {L}earned {T}o {S}top {W}orrying {A}nd {L}ove {R}etraining}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=_nF5imFKQI}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="optimization"> <div class="pub-thumb-col" data-tags="optimization"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/nashflow.png" data-zoomable=""> </div></div> <div id="ziemke2021flows" class="pub-content-col"> <div class="title"> <a href="https://www.sciencedirect.com/science/article/pii/S2352146521000284" class="no-highlight" rel="external nofollow noopener" target="_blank"> Flows over time as continuous limits of packet-based network simulations </a> </div> <div class="author"> T. Ziemke, L. Sering, <a href="https://sites.google.com/view/lvargaskoch" rel="external nofollow noopener" target="_blank">L. V. Koch</a>, <em>M. Zimmer</em>, K. Nagel, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Martin Skutella' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '0'); ">1 more author</span> </div> <div class="periodical"> <abbr class="badge">Journal</abbr><span class="venue-name"><em>Transportation Research Procedia</em></span><span class="year"> 2021</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S2352146521000284" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This study examines the connection between an agent-based transport simulation and Nash flows over time. While the former is able to represent many details of traffic and model large-scale, real-world traffic situations with a co-evolutionary approach, the latter provides an environment for provable mathematical statements and results on exact user equilibria. The flow dynamics of both models are very similar with the main difference that the simulation is discrete in terms of vehicles and time while the flows over time model considers continuous flows and continuous time. This raises the question whether Nash flows over time are the limit of the convergence process when decreasing the vehicle and time step size in the simulation coherently. The experiments presented in this study indicate this strong connection which provides a justification for the analytical model and a theoretical foundation for the simulation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ziemke2021flows</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Flows over time as continuous limits of packet-based network simulations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ziemke, Theresa and Sering, Leon and Koch, Laura Vargas and Zimmer, Max and Nagel, Kai and Skutella, Martin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transportation Research Procedia}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{52}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{123--130}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="optimization"> <div class="pub-thumb-col" data-tags="optimization"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/fw.png" data-zoomable=""> </div></div> <div id="pokutta2020deep" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2010.07243" class="no-highlight" rel="external nofollow noopener" target="_blank"> Deep Neural Network training with Frank-Wolfe </a> </div> <div class="author"> <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a>, <a href="http://christophspiegel.berlin/" rel="external nofollow noopener" target="_blank">C. Spiegel</a>, and <em>M. Zimmer</em> </div> <div class="periodical"> <abbr class="badge">Preprint</abbr><span class="venue-name"><em>arXiv preprint arXiv:2010.07243</em></span><span class="year"> 2020</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2010.07243" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.pokutta.com/blog/research/2020/11/11/NNFW.html" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/ZIB-IOL/StochasticFrankWolfe" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This paper studies the empirical efficacy and benefits of using projection-free first-order methods in the form of Conditional Gradients, a.k.a. Frank-Wolfe methods, for training Neural Networks with constrained parameters. We draw comparisons both to current state-of-the-art stochastic Gradient Descent methods as well as across different variants of stochastic Conditional Gradients. In particular, we show the general feasibility of training Neural Networks whose parameters are constrained by a convex feasible region using Frank-Wolfe algorithms and compare different stochastic variants. We then show that, by choosing an appropriate region, one can achieve performance exceeding that of unconstrained stochastic Gradient Descent and matching state-of-the-art results relying on L^2-regularization. Lastly, we also demonstrate that, besides impacting performance, the particular choice of constraints can have a drastic impact on the learned representations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pokutta2020deep</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Neural Network training with Frank-Wolfe}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pokutta, Sebastian and Spiegel, Christoph and Zimmer, Max}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2010.07243}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright <script>document.write((new Date).getFullYear());</script> Max Zimmer · <a href="https://maxzimmer.org/legal-notice">Legal Notice</a> · <a href="https://maxzimmer.org/privacy-policy">Privacy Policy</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"]]}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/publications.js"></script> </body> </html>