<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications tagged "Efficiency" | Max Zimmer</title> <meta name="author" content="Max Zimmer"> <meta name="description" content="Academic website of Max Zimmer. Deep Learning Research Lead at the IOL Lab of ZIB. "> <meta name="keywords" content="Max Zimmer, mathematics, machine learning, deep learning, optimization, neural network pruning, sparsity, quantization, AI4Science, sustainability, TU Berlin, Zuse Institute Berlin"> <meta property="og:site_name" content="Max Zimmer"> <meta property="og:type" content="website"> <meta property="og:title" content="Max Zimmer | Publications tagged " efficiency> <meta property="og:url" content="https://maxzimmer.org/publications/tag/efficiency/"> <meta property="og:description" content="Academic website of Max Zimmer. Deep Learning Research Lead at the IOL Lab of ZIB. "> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Publications tagged " efficiency> <meta name="twitter:description" content="Academic website of Max Zimmer. Deep Learning Research Lead at the IOL Lab of ZIB. "> <meta name="twitter:site" content="@maxzimmerberlin"> <meta name="twitter:creator" content="@maxzimmerberlin"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://maxzimmer.org/publications/tag/efficiency/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script defer data-domain="maxzimmer.org" src="https://plausible.iol.zib.de/js/script.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Max Zimmer</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks &amp; activities</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/ai_news/">ai news &amp; links</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications tagged "efficiency"</h1> </header> <article> <div class="publications"> <ol class="bibliography"> <li> <div class="row entry" data-tags="efficiency"> <div class="pub-thumb-col" data-tags="efficiency"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/recon.png" data-zoomable> </div></div> <div id="urbano2025recon" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2505.13289" class="no-highlight" rel="external nofollow noopener" target="_blank"> RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization </a> </div> <div class="author"> A. Urbano, <a href="https://davidwromero.xyz" rel="external nofollow noopener" target="_blank">D. W. Romero</a>, <em>M. Zimmer</em>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">ICLR26</abbr><span class="venue-name"> <em>The Fourteenth International Conference on Learning Representations</em> </span><span class="year"> 2026</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2505.13289" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Real-world data often exhibits unknown or approximate symmetries, yet existing equivariant networks must commit to a fixed transformation group prior to training, e.g., continuous SO(2) rotations. This mismatch degrades performance when the actual data symmetries differ from those in the transformation group. We introduce RECON, a framework to discover each input’s intrinsic symmetry distribution from unlabeled data. RECON leverages class-pose decompositions and applies a data-driven normalization to align arbitrary reference frames into a common natural pose, yielding directly comparable and interpretable symmetry descriptors. We demonstrate effective symmetry discovery on 2D image benchmarks and – for the first time – extend it to 3D transformation groups, paving the way towards more flexible equivariant modeling.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">urbano2025recon</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Urbano, Alonso and Romero, David W and Zimmer, Max and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Fourteenth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency"> <div class="pub-thumb-col" data-tags="efficiency"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/sparseswaps.png" data-zoomable> </div></div> <div id="zimmer2025sparseswaps" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2512.10922" class="no-highlight" rel="external nofollow noopener" target="_blank"> SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale </a> </div> <div class="author"> <em>M. Zimmer</em>, <a href="http://christopheroux.de/" rel="external nofollow noopener" target="_blank">C. Roux</a>, M. Wagner, D. Hendrych, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">Preprint</abbr><span class="venue-name"><em>arXiv preprint arXiv:2512.10922</em></span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2512.10922" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>The resource requirements of Neural Networks can be significantly reduced through pruning – the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zimmer2025sparseswaps</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zimmer, Max and Roux, Christophe and Wagner, Moritz and Hendrych, Deborah and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2512.10922}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency"> <div class="pub-thumb-col" data-tags="efficiency"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/llm_compression.png" data-zoomable> </div></div> <div id="wagner2025freelunchllmcompression" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2510.14444" class="no-highlight" rel="external nofollow noopener" target="_blank"> A Free Lunch in LLM Compression: Revisiting Retraining after Pruning </a> </div> <div class="author"> M. Wagner, <a href="http://christopheroux.de/" rel="external nofollow noopener" target="_blank">C. Roux</a>, <em>M. Zimmer</em>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">Preprint</abbr><span class="venue-name"><em>arXiv preprint arXiv:2510.14444</em></span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2510.14444" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>While Neural Network pruning typically requires retraining the model to recover pruning-induced performance degradation, state-of-the-art Large Language Models (LLMs) pruning methods instead solve a layer-wise mask selection and reconstruction problem on a small set of calibration data to avoid full retraining, as it is considered computationally infeasible for LLMs. Reconstructing single matrices in isolation has favorable properties, such as convexity of the objective and significantly reduced memory requirements compared to full retraining. In practice, however, reconstruction is often implemented at coarser granularities, e.g., reconstructing a whole transformer block against its dense activations instead of a single matrix. In this work, we study the key design choices when reconstructing or retraining the remaining weights after pruning. We conduct an extensive computational study on state-of-the-art GPT architectures, and report several surprising findings that challenge common intuitions about retraining after pruning. In particular, we observe a free lunch scenario: reconstructing attention and MLP components separately within each transformer block is nearly the most resource-efficient yet achieves the best perplexity. Most importantly, this Pareto-optimal setup achieves better performance than full retraining, despite requiring only a fraction of the memory. Furthermore, we demonstrate that simple and efficient pruning criteria such as Wanda can outperform much more complex approaches when the reconstruction step is properly executed, highlighting its importance. Our findings challenge the narrative that retraining should be avoided at all costs and provide important insights into post-pruning performance recovery for LLMs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wagner2025freelunchllmcompression</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Free Lunch in LLM Compression: Revisiting Retraining after Pruning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wagner, Moritz and Roux, Christophe and Zimmer, Max and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2510.14444}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency, optimization"> <div class="pub-thumb-col" data-tags="efficiency, optimization"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/frankwolfe_pruning.png" data-zoomable> </div></div> <div id="roux2025dontbegreedyjustrelax" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2510.13713" class="no-highlight" rel="external nofollow noopener" target="_blank"> Don’t Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe </a> </div> <div class="author"> <a href="http://christopheroux.de/" rel="external nofollow noopener" target="_blank">C. Roux</a>, <em>M. Zimmer</em>, A. d’Aspremont, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">Preprint</abbr><span class="venue-name"><em>arXiv preprint arXiv:2510.13713</em></span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2510.13713" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Pruning is a common technique to reduce the compute and storage requirements of Neural Networks. While conventional approaches typically retrain the model to recover pruning-induced performance degradation, state-of-the-art Large Language Model (LLM) pruning methods operate layer-wise, minimizing the per-layer pruning error on a small calibration dataset to avoid full retraining, which is considered computationally prohibitive for LLMs. However, finding the optimal pruning mask is a hard combinatorial problem and solving it to optimality is intractable. Existing methods hence rely on greedy heuristics that ignore the weight interactions in the pruning objective. In this work, we instead consider the convex relaxation of these combinatorial constraints and solve the resulting problem using the Frank-Wolfe (FW) algorithm. Our method drastically reduces the per-layer pruning error, outperforms strong baselines on state-of-the-art GPT architectures, and remains memory-efficient. We provide theoretical justification by showing that, combined with the convergence guarantees of the FW algorithm, we obtain an approximate solution to the original combinatorial problem upon rounding the relaxed solution to integrality.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">roux2025dontbegreedyjustrelax</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Roux, Christophe and Zimmer, Max and d'Aspremont, Alexandre and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2510.13713}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency, computational-algebra"> <div class="pub-thumb-col" data-tags="efficiency, computational-algebra"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/vanishingideals.png" data-zoomable> </div></div> <div id="pelleriti2025approximatinglatentmanifoldsneural" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2502.15051" class="no-highlight" rel="external nofollow noopener" target="_blank"> Approximating Latent Manifolds in Neural Networks via Vanishing Ideals </a> </div> <div class="author"> N. Pelleriti, <em>M. Zimmer</em>, <a href="https://elwirth.github.io" rel="external nofollow noopener" target="_blank">E. Wirth</a>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">ICML25</abbr><span class="venue-name"> <em>Forty-second International Conference on Machine Learning</em> </span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2502.15051" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="blog/2025/approximating-latent-manifolds-in-neural-networks" class="btn btn-sm" role="button">Blog</a> <a href="https://github.com/ZIB-IOL/approximating-neural-network-manifolds" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/ApproximatingLatentManifolds-poster-ICML.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Deep neural networks have reshaped modern machine learning by learning powerful latent representations that often align with the manifold hypothesis: high-dimensional data lie on lower-dimensional manifolds. In this paper, we establish a connection between manifold learning and computational algebra by demonstrating how vanishing ideals can characterize the latent manifolds of deep networks. To that end, we propose a new neural architecture that (i) truncates a pretrained network at an intermediate layer, (ii) approximates each class manifold via polynomial generators of the vanishing ideal, and (iii) transforms the resulting latent space into linearly separable features through a single polynomial layer. The resulting models have significantly fewer layers than their pretrained baselines, while maintaining comparable accuracy, achieving higher throughput, and utilizing fewer parameters. Furthermore, drawing on spectral complexity analysis, we derive sharper theoretical guarantees for generalization, showing that our approach can in principle offer tighter bounds than standard deep networks. Numerical experiments confirm the effectiveness and efficiency of the proposed approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pelleriti2025approximatinglatentmanifoldsneural</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Approximating Latent Manifolds in Neural Networks via Vanishing Ideals}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pelleriti, Nico and Zimmer, Max and Wirth, Elias and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Forty-second International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=WYlerYGDPL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency, optimization"> <div class="pub-thumb-col" data-tags="efficiency, optimization"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/sparsefw_pruneddistance.png" data-zoomable> </div></div> <div id="ZimmerSpiegelPokutta+2025+137+168" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2205.11921" class="no-highlight" rel="external nofollow noopener" target="_blank"> Compression-aware Training of Neural Networks using Frank-Wolfe </a> </div> <div class="author"> <em>M. Zimmer</em>, <a href="http://christophspiegel.berlin/" rel="external nofollow noopener" target="_blank">C. Spiegel</a>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">Journal</abbr><span class="venue-name"> <em>Mathematical Optimization for Machine Learning</em> </span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2205.11921" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ZIB-IOL/compression-aware-SFW" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Many existing Neural Network pruning approaches either rely on retraining to compensate for pruning-caused performance degradation or they induce strong biases to converge to a specific sparse solution throughout training. A third paradigm obtains a wide range of compression ratios from a single dense training run while also avoiding retraining. Recent work of Pokutta et al. (2020) and Miao et al. (2022) suggests that the Stochastic Frank-Wolfe (SFW) algorithm is particularly suited for training state-of-the-art models that are robust to compression. We propose leveraging k-support norm ball constraints and demonstrate significant improvements over the results of Miao et al. (2022) in the case of unstructured pruning. We also extend these ideas to the structured pruning domain and propose novel approaches to both ensure robustness to the pruning of convolutional filters as well as to low-rank tensor decompositions of convolutional layers. In the latter case, our approach performs on-par with nuclear-norm regularization baselines while requiring only half of the computational resources. Our findings also indicate that the robustness of SFW-trained models largely depends on the gradient rescaling of the learning rate and we establish a theoretical foundation for that practice.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inbook</span><span class="p">{</span><span class="nl">ZimmerSpiegelPokutta+2025+137+168</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1515/9783111376776-010}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Compression-aware Training of Neural Networks using Frank-Wolfe}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Mathematical Optimization for Machine Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zimmer, Max and Spiegel, Christoph and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Fackeldey, Konstantin and Kannan, Aswin and Pokutta, Sebastian and Sharma, Kartikey and Walter, Daniel and Walther, Andrea and Weiser, Martin}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{De Gruyter}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Berlin, Boston}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{137--168}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{doi:10.1515/9783111376776-010}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9783111376776}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency"> <div class="pub-thumb-col" data-tags="efficiency"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/cs256.png" data-zoomable> </div></div> <div id="lasby2025compressed" class="pub-content-col"> <div class="title"> <a href="https://openreview.net/forum?id=iso0KV2HVq" class="no-highlight" rel="external nofollow noopener" target="_blank"> Compressed Sparse Tiles for Memory-Efficient Unstructured and Semi-Structured Sparsity </a> </div> <div class="author"> M. Lasby, <em>M. Zimmer</em>, <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a>, and E. Schultheis</div> <div class="periodical"> <abbr class="badge">Workshop</abbr><span class="venue-name"> <em>ICLR25 Workshop on Sparsity in LLMs (SLLM)</em> </span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://openreview.net/forum?id=iso0KV2HVq" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/CS256-poster-ICLR.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Storing the weights of large language models (LLMs) in GPU memory for local inference is challenging due to their size. While quantization has proven successful in reducing the memory footprint of LLMs, unstructured pruning introduces overhead by requiring the non-pruned weights’ location to be encoded. This overhead hinders the efficient combination of quantization and unstructured pruning, especially for smaller batch sizes common in inference scenarios. To address this, we propose the CS256 storage format, which offers a better balance between space efficiency and hardware acceleration compared to existing formats. CS256 partitions the weight matrix into tiles and uses a hierarchical indexing scheme to locate non-zero values, reducing the overhead associated with storing sparsity patterns. Our preliminary results with one-shot pruning of LLMs show that CS256 matches the performance of unstructured sparsity while being more hardware-friendly.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lasby2025compressed</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Compressed Sparse Tiles for Memory-Efficient Unstructured and Semi-Structured Sparsity}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lasby, Mike and Zimmer, Max and Pokutta, Sebastian and Schultheis, Erik}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Sparsity in LLMs (SLLM): Deep Dive into Mixture of Experts, Quantization, Hardware, and Inference}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=iso0KV2HVq}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency"> <div class="pub-thumb-col" data-tags="efficiency"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/sms_algo_sketch.png" data-zoomable> </div></div> <div id="zimmer2023sparse" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2306.16788" class="no-highlight" rel="external nofollow noopener" target="_blank"> Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging </a> </div> <div class="author"> <em>M. Zimmer</em>, <a href="http://christophspiegel.berlin/" rel="external nofollow noopener" target="_blank">C. Spiegel</a>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">ICLR24</abbr><span class="venue-name"> <em>The Twelfth International Conference on Learning Representations</em> </span><span class="year"> 2024</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2306.16788" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.pokutta.com/blog/research/2023/08/05/abstract-SMS.html" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/ZIB-IOL/SMS" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/SMS-poster-ICLR.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Neural networks can be significantly compressed by pruning, leading to sparse models requiring considerably less storage and floating-point operations while maintaining predictive performance. Model soups (Wortsman et al., 2022) improve generalization and out-of-distribution performance by averaging the parameters of multiple models into a single one without increased inference time. However, identifying models in the same loss basin to leverage both sparsity and parameter averaging is challenging, as averaging arbitrary sparse models reduces the overall sparsity due to differing sparse connectivities. In this work, we address these challenges by demonstrating that exploring a single retraining phase of Iterative Magnitude Pruning (IMP) with varying hyperparameter configurations, such as batch ordering or weight decay, produces models that are suitable for averaging and share the same sparse connectivity by design. Averaging these models significantly enhances generalization performance compared to their individual components. Building on this idea, we introduce Sparse Model Soups (SMS), a novel method for merging sparse models by initiating each prune-retrain cycle with the averaged model of the previous phase. SMS maintains sparsity, exploits sparse network benefits being modular and fully parallelizable, and substantially improves IMP’s performance. Additionally, we demonstrate that SMS can be adapted to enhance the performance of state-of-the-art pruning during training approaches.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zimmer2023sparse</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zimmer, Max and Spiegel, Christoph and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency"> <div class="pub-thumb-col" data-tags="efficiency"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/perp_dog.png" data-zoomable> </div></div> <div id="zimmer2023perp" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2312.15230" class="no-highlight" rel="external nofollow noopener" target="_blank"> PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs </a> </div> <div class="author"> <em>M. Zimmer</em>, M. Andoni, <a href="http://christophspiegel.berlin/" rel="external nofollow noopener" target="_blank">C. Spiegel</a>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">Preprint</abbr><span class="venue-name"><em>arXiv preprint arXiv:2312.15230</em></span><span class="year"> 2023</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2312.15230" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ZIB-IOL/PERP" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Neural Networks can be effectively compressed through pruning, significantly reducing storage and compute demands while maintaining predictive performance. Simple yet effective methods like magnitude pruning remove less important parameters and typically require a costly retraining procedure to restore performance. However, with the rise of LLMs, full retraining has become infeasible due to memory and compute constraints. This study challenges the practice of retraining all parameters by showing that updating a small subset of highly expressive parameters can suffice to recover or even enhance performance after pruning. Surprisingly, retraining just 0.01%-0.05% of the parameters in GPT-architectures can match the performance of full retraining across various sparsity levels, significantly reducing compute and memory requirements, and enabling retraining of models with up to 30 billion parameters on a single GPU in minutes. To bridge the gap to full retraining in the high sparsity regime, we introduce two novel LoRA variants that, unlike standard LoRA, allow merging adapters back without compromising sparsity. Going a step further, we show that these methods can be applied for memory-efficient layer-wise reconstruction, significantly enhancing state-of-the-art retraining-free methods like Wanda (Sun et al., 2023) and SparseGPT (Frantar &amp; Alistarh, 2023). Our findings present a promising alternative to avoiding retraining.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zimmer2023perp</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zimmer, Max and Andoni, Megi and Spiegel, Christoph and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2312.15230}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="efficiency"> <div class="pub-thumb-col" data-tags="efficiency"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/bimp_lr_schedules.png" data-zoomable> </div></div> <div id="Zimmer2023" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2111.00843" class="no-highlight" rel="external nofollow noopener" target="_blank"> How I Learned To Stop Worrying And Love Retraining </a> </div> <div class="author"> <em>M. Zimmer</em>, <a href="http://christophspiegel.berlin/" rel="external nofollow noopener" target="_blank">C. Spiegel</a>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">ICLR23</abbr><span class="venue-name"> <em>The Eleventh International Conference on Learning Representations</em> </span><span class="year"> 2023</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2111.00843" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.pokutta.com/blog/research/2023/07/21/abstract-BIMP.html" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/ZIB-IOL/BIMP" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/BIMP-poster-ICLR.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Many Neural Network Pruning approaches consist of several iterative training and pruning steps, seemingly losing a significant amount of their performance after pruning and then recovering it in the subsequent retraining phase. Recent works of Renda et al. (2020) and Le &amp; Hua (2021) demonstrate the significance of the learning rate schedule during the retraining phase and propose specific heuristics for choosing such a schedule for IMP (Han et al., 2015). We place these findings in the context of the results of Li et al. (2020) regarding the training of models within a fixed training budget and demonstrate that, consequently, the retraining phase can be massively shortened using a simple linear learning rate schedule. Improving on existing retraining approaches, we additionally propose a method to adaptively select the initial value of the linear schedule. Going a step further, we propose similarly imposing a budget on the initial dense training phase and show that the resulting simple and efficient method is capable of outperforming significantly more complex or heavily parameterized state-of-the-art approaches that attempt to sparsify the network during training. These findings not only advance our understanding of the retraining phase, but more broadly question the belief that one should aim to avoid the need for retraining and reduce the negative effects of ‘hard’ pruning by incorporating the sparsification process into the standard training.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Zimmer2023</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zimmer, Max and Spiegel, Christoph and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Eleventh International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{H}ow {I} {L}earned {T}o {S}top {W}orrying {A}nd {L}ove {R}etraining}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=_nF5imFKQI}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright <script>document.write((new Date).getFullYear());</script> Max Zimmer · <a href="https://maxzimmer.org/legal-notice">Legal Notice</a> · <a href="https://maxzimmer.org/privacy-policy">Privacy Policy</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"]]}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/publications.js"></script> </body> </html>