<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications tagged "AI4Math" | Max Zimmer</title> <meta name="author" content="Max Zimmer"> <meta name="description" content="Academic website of Max Zimmer. Deep Learning Research Lead at the IOL Lab of ZIB. "> <meta name="keywords" content="Max Zimmer, mathematics, machine learning, deep learning, optimization, neural network pruning, sparsity, quantization, AI4Science, sustainability, TU Berlin, Zuse Institute Berlin"> <meta property="og:site_name" content="Max Zimmer"> <meta property="og:type" content="website"> <meta property="og:title" content="Max Zimmer | Publications tagged " ai4math> <meta property="og:url" content="https://maxzimmer.org/publications/tag/ai4math/"> <meta property="og:description" content="Academic website of Max Zimmer. Deep Learning Research Lead at the IOL Lab of ZIB. "> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Publications tagged " ai4math> <meta name="twitter:description" content="Academic website of Max Zimmer. Deep Learning Research Lead at the IOL Lab of ZIB. "> <meta name="twitter:site" content="@maxzimmerberlin"> <meta name="twitter:creator" content="@maxzimmerberlin"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://maxzimmer.org/publications/tag/ai4math/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script defer data-domain="maxzimmer.org" src="https://plausible.iol.zib.de/js/script.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Max Zimmer</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks &amp; activities</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/ai_news/">ai news &amp; links</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications tagged "ai4math"</h1> </header> <article> <div class="publications"> <ol class="bibliography"> <li> <div class="row entry" data-tags="ai4math, algebraic-geometry"> <div class="pub-thumb-col" data-tags="ai4math, algebraic-geometry"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/patchworked_curves.png" data-zoomable> </div></div> <div id="geiselmann2026patchworked" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2602.06888" class="no-highlight" rel="external nofollow noopener" target="_blank"> 121 Patchworked Curves of Degree Seven </a> </div> <div class="author"> Z. Geiselmann, <a href="https://page.math.tu-berlin.de/~joswig/" rel="external nofollow noopener" target="_blank">M. Joswig</a>, L. Kastner, <a href="https://iol.zib.de/team/konrad-mundinger.html" rel="external nofollow noopener" target="_blank">K. Mundinger</a>, <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Christoph Spiegel, Marcel Wack, Max Zimmer' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '0'); ">3 more authors</span> </div> <div class="periodical"> <abbr class="badge">Preprint</abbr><span class="venue-name"><em>arXiv preprint arXiv:2602.06888</em></span><span class="year"> 2026</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2602.06888" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We classify all 121 real schemes of smooth real plane algebraic curves of degree seven by constructing patchworks. We provide an explicit method for constructing polynomials realizing each real scheme. The work demonstrates that every real scheme at this degree level can be realized as a T-curve, resolving a previously open question from 1996.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">geiselmann2026patchworked</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{121 Patchworked Curves of Degree Seven}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Geiselmann, Zoe and Joswig, Michael and Kastner, Lars and Mundinger, Konrad and Pokutta, Sebastian and Spiegel, Christoph and Wack, Marcel and Zimmer, Max}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2602.06888}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="computational-algebra, ai4math, ai4science"> <div class="pub-thumb-col" data-tags="computational-algebra, ai4math, ai4science"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/neural_sos.png" data-zoomable> </div></div> <div id="pelleriti2025neural" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2510.13444" class="no-highlight" rel="external nofollow noopener" target="_blank"> Neural Sum-of-Squares: Certifying the Nonnegativity of Polynomials with Transformers </a> </div> <div class="author"> N. Pelleriti, <a href="http://christophspiegel.berlin/" rel="external nofollow noopener" target="_blank">C. Spiegel</a>, <a href="https://shiweiliuiiiiiii.github.io" rel="external nofollow noopener" target="_blank">S. Liu</a>, <a href="https://damaru2.github.io" rel="external nofollow noopener" target="_blank">D. Martínez-Rubio</a>, <em>M. Zimmer</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sebastian Pokutta' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '0'); ">1 more author</span> </div> <div class="periodical"> <abbr class="badge">ICLR26</abbr><span class="venue-name"> <em>The Fourteenth International Conference on Learning Representations</em> </span><span class="year"> 2026</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2510.13444" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ZIB-IOL/Neural-Sum-of-Squares" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Certifying nonnegativity of polynomials is a well-known NP-hard problem with direct applications spanning non-convex optimization, control, robotics, and beyond. A sufficient condition for nonnegativity is the Sum of Squares (SOS) property, i.e., it can be written as a sum of squares of other polynomials. In practice, however, certifying the SOS criterion remains computationally expensive and often involves solving a Semidefinite Program (SDP), whose dimensionality grows quadratically in the size of the monomial basis of the SOS expression; hence, various methods to reduce the size of the monomial basis have been proposed. In this work, we introduce the first learning-augmented algorithm to certify the SOS criterion. To this end, we train a Transformer model that predicts an almost-minimal monomial basis for a given polynomial, thereby drastically reducing the size of the corresponding SDP. Our overall methodology comprises three key components: efficient training dataset generation of over 100 million SOS polynomials, design and training of the corresponding Transformer architecture, and a systematic fallback mechanism to ensure correct termination, which we analyze theoretically. We validate our approach on over 200 benchmark datasets, achieving speedups of over 100× compared to state-of-the-art solvers and enabling the solution of instances where competing approaches fail. Our findings provide novel insights towards transforming the practical scalability of SOS programming.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pelleriti2025neural</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural Sum-of-Squares: Certifying the Nonnegativity of Polynomials with Transformers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pelleriti, Nico and Spiegel, Christoph and Liu, Shiwei and Mart{\'i}nez-Rubio, David and Zimmer, Max and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Fourteenth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="ai4science, computational-algebra, ai4math"> <div class="pub-thumb-col" data-tags="ai4science, computational-algebra, ai4math"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/obba.png" data-zoomable> </div></div> <div id="kera2025computationalalgebraattentiontransformer" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2505.23696" class="no-highlight" rel="external nofollow noopener" target="_blank"> Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms </a> </div> <div class="author"> H. Kera, N. Pelleriti, Y. Ishihara, <em>M. Zimmer</em>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">NEURIPS25</abbr><span class="venue-name"> <em>Advances in Neural Information Processing Systems</em> </span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2505.23696" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/HiroshiKERA/OracleBorderBasis" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/OBBA-poster-NeurIPS.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Solving systems of polynomial equations, particularly those with finitely many solutions, is a crucial challenge across many scientific fields. Traditional methods like Gröbner and Border bases are fundamental but suffer from high computational costs, which have motivated recent Deep Learning approaches to improve efficiency, albeit at the expense of output correctness. In this work, we introduce the Oracle Border Basis Algorithm, the first Deep Learning approach that accelerates Border basis computation while maintaining output guarantees. To this end, we design and train a Transformer-based oracle that identifies and eliminates computationally expensive reduction steps, which we find to dominate the algorithm’s runtime. By selectively invoking this oracle during critical phases of computation, we achieve substantial speedup factors of up to 3.5x compared to the base algorithm, without compromising the correctness of results. To generate the training data, we develop a sampling method and provide the first sampling theorem for border bases. We construct a tokenization and embedding scheme tailored to monomial-centered algebraic computations, resulting in a compact and expressive input representation, which reduces the number of tokens to encode an n-variate polynomial by a factor of O(n). Our learning approach is data efficient, stable, and a practical enhancement to traditional computer algebra algorithms and symbolic computation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kera2025computationalalgebraattentiontransformer</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kera, Hiroshi and Pelleriti, Nico and Ishihara, Yuki and Zimmer, Max and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{38}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="ai4science, ai4math"> <div class="pub-thumb-col" data-tags="ai4science, ai4math"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/colordreaming.png" data-zoomable> </div></div> <div id="mundinger2025neural" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2501.18527" class="no-highlight" rel="external nofollow noopener" target="_blank"> Neural Discovery in Mathematics: Do Machines Dream of Colored Planes? </a> </div> <div class="author"> <a href="https://iol.zib.de/team/konrad-mundinger.html" rel="external nofollow noopener" target="_blank">K. Mundinger</a>, <em>M. Zimmer</em>, A. Kiem, <a href="http://christophspiegel.berlin/" rel="external nofollow noopener" target="_blank">C. Spiegel</a>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">ICML25</abbr><span class="venue-name"> <em>Forty-second International Conference on Machine Learning (Oral presentation, top 1%)</em> </span><span class="year"> 2025</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2501.18527" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="blog/2025/neural-discovery-in-mathematics-do-machines-dream-of-colored-planes" class="btn btn-sm" role="button">Blog</a> <a href="https://github.com/ZIB-IOL/neural-discovery-icml25" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/NeuralDiscovery-poster-ICML.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We demonstrate how neural networks can drive mathematical discovery through a case study of the Hadwiger-Nelson problem, a long-standing open problem from discrete geometry and combinatorics about coloring the plane avoiding monochromatic unit-distance pairs. Using neural networks as approximators, we reformulate this mixed discrete-continuous geometric coloring problem as an optimization task with a probabilistic, differentiable loss function. This enables gradient-based exploration of admissible configurations that most significantly led to the discovery of two novel six-colorings, providing the first improvements in thirty years to the off-diagonal variant of the original problem (Mundinger et al., 2024a). Here, we establish the underlying machine learning approach used to obtain these results and demonstrate its broader applicability through additional results and numerical insights.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mundinger2025neural</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural Discovery in Mathematics: Do Machines Dream of Colored Planes?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mundinger, Konrad and Zimmer, Max and Kiem, Aldo and Spiegel, Christoph and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Forty-second International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=7Tp9zjP9At}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="ai4science, ai4math"> <div class="pub-thumb-col" data-tags="ai4science, ai4math"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/hadwigernelson.png" data-zoomable> </div></div> <div id="mundinger2024extending" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2404.05509" class="no-highlight" rel="external nofollow noopener" target="_blank"> Extending the Continuum of Six-Colorings </a> </div> <div class="author"> <a href="https://iol.zib.de/team/konrad-mundinger.html" rel="external nofollow noopener" target="_blank">K. Mundinger</a>, <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a>, <a href="http://christophspiegel.berlin/" rel="external nofollow noopener" target="_blank">C. Spiegel</a>, and <em>M. Zimmer</em> </div> <div class="periodical"> <abbr class="badge">Journal</abbr><span class="venue-name"><em>Geombinatorics Quarterly</em></span><span class="year"> 2024</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2404.05509" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.pokutta.com/blog/research/2024/07/28/hadwiger-nelson.html" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Blog</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We present two novel six-colorings of the Euclidean plane that avoid monochromatic pairs of points at unit distance in five colors and monochromatic pairs at another specified distance d in the sixth color. Such colorings have previously been known to exist for 0.41 &lt; \sqrt2 - 1 \le d \le 1 / \sqrt5 &lt; 0.45. Our results significantly expand that range to 0.354 \le d \le 0.657, the first improvement in 30 years. Notably, the constructions underlying this were derived by formalizing colorings suggested by a custom machine learning approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mundinger2024extending</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mundinger, Konrad and Pokutta, Sebastian and Spiegel, Christoph and Zimmer, Max}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Geombinatorics Quarterly}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Extending the Continuum of Six-Colorings}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{XXXIV}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2404.05509}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://geombina.uccs.edu/past-issues/volume-xxxiv}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row entry" data-tags="ai4science, ai4math"> <div class="pub-thumb-col" data-tags="ai4science, ai4math"><div class="preview"> <img class="preview rounded" src="/assets/img/publication_preview/pde.png" data-zoomable> </div></div> <div id="mundinger2024neural" class="pub-content-col"> <div class="title"> <a href="https://arxiv.org/abs/2403.12764" class="no-highlight" rel="external nofollow noopener" target="_blank"> Neural Parameter Regression for Explicit Representations of PDE Solution Operators </a> </div> <div class="author"> <a href="https://iol.zib.de/team/konrad-mundinger.html" rel="external nofollow noopener" target="_blank">K. Mundinger</a>, <em>M. Zimmer</em>, and <a href="https://pokutta.com" rel="external nofollow noopener" target="_blank">S. Pokutta</a> </div> <div class="periodical"> <abbr class="badge">Workshop</abbr><span class="venue-name"> <em>ICLR24 Workshop on AI4DifferentialEquations In Science</em> </span><span class="year"> 2024</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://arxiv.org/abs/2403.12764" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/NPR-poster-ICLR.pdf" class="btn btn-sm" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We introduce Neural Parameter Regression (NPR), a novel framework specifically developed for learning solution operators in Partial Differential Equations (PDEs). Tailored for operator learning, this approach surpasses traditional DeepONets (Lu et al., 2021) by employing Physics-Informed Neural Network (PINN, Raissi et al., 2019) techniques to regress Neural Network (NN) parameters. By parametrizing each solution based on specific initial conditions, it effectively approximates a mapping between function spaces. Our method enhances parameter efficiency by incorporating low-rank matrices, thereby boosting computational efficiency and scalability. The framework shows remarkable adaptability to new initial and boundary conditions, allowing for rapid fine-tuning and inference, even in cases of out-of-distribution examples.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mundinger2024neural</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mundinger, Konrad and Zimmer, Max and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural Parameter Regression for Explicit Representations of PDE Solution Operators}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICLR 2024 Workshop on AI4DifferentialEquations In Science}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=6Z0q0dzSJQ}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright <script>document.write((new Date).getFullYear());</script> Max Zimmer · <a href="https://maxzimmer.org/legal-notice">Legal Notice</a> · <a href="https://maxzimmer.org/privacy-policy">Privacy Policy</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"]]}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/publications.js"></script> </body> </html>