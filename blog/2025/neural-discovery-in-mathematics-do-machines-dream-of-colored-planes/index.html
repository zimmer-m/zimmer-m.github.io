<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Neural Discovery in Mathematics: Do Machines Dream of Colored Planes? | Max Zimmer</title> <meta name="author" content="Max Zimmer"> <meta name="description" content="Our ICML 2025 Oral Paper &lt;b&gt;Neural Discovery in Mathematics: Do Machines Dream of Colored Planes?&lt;/b&gt; introduces a novel neural network approach to tackle the famous Hadwiger-Nelson problem and related geometric coloring challenges. We reformulate the combinatorial task as a continuous optimization problem, enabling neural networks to find probabilistic colorings. This led to discovering two new 6-colorings, marking the first progress in 30 years on a key variant involving different forbidden distances and significantly expanding the known solution range."> <meta name="keywords" content="Max Zimmer, mathematics, machine learning, optimization"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://maxzimmer.org/blog/2025/neural-discovery-in-mathematics-do-machines-dream-of-colored-planes/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"]]}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Neural Discovery in Mathematics: Do Machines Dream of Colored Planes?",
      "description": "Our ICML 2025 Oral Paper <b>Neural Discovery in Mathematics: Do Machines Dream of Colored Planes?</b> introduces a novel neural network approach to tackle the famous Hadwiger-Nelson problem and related geometric coloring challenges. We reformulate the combinatorial task as a continuous optimization problem, enabling neural networks to find probabilistic colorings. This led to discovering two new 6-colorings, marking the first progress in 30 years on a key variant involving different forbidden distances and significantly expanding the known solution range.",
      "published": "May 29, 2025",
      "authors": [
        {
          "author": "Konrad Mundinger",
          
          
            
            "authorURL": "https://iol.zib.de/team/konrad-mundinger.html",
          
          "affiliations": [
            {
              "name": "Zuse Institute Berlin",
              "url": ""
            }
          ]
        },
        {
          "author": "Max Zimmer",
          
          
            "authorURL": "",
          
          "affiliations": [
            {
              "name": "Zuse Institute Berlin",
              "url": ""
            }
          ]
        },
        {
          "author": "Aldo Kiem",
          
          
            "authorURL": "",
          
          "affiliations": [
            {
              "name": "Zuse Institute Berlin",
              "url": ""
            }
          ]
        },
        {
          "author": "Christoph Spiegel",
          
          
            
            "authorURL": "http://christophspiegel.berlin/",
          
          "affiliations": [
            {
              "name": "Zuse Institute Berlin",
              "url": ""
            }
          ]
        },
        {
          "author": "Sebastian Pokutta",
          
          
            
            "authorURL": "https://pokutta.com",
          
          "affiliations": [
            {
              "name": "Zuse Institute Berlin",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Max Zimmer</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks &amp; activities</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/ai_news/">AI news</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Neural Discovery in Mathematics: Do Machines Dream of Colored Planes?</h1> <p style="margin-bottom: 0.1em;">Our ICML 2025 Oral Paper <b>Neural Discovery in Mathematics: Do Machines Dream of Colored Planes?</b> introduces a novel neural network approach to tackle the famous Hadwiger-Nelson problem and related geometric coloring challenges. We reformulate the combinatorial task as a continuous optimization problem, enabling neural networks to find probabilistic colorings. This led to discovering two new 6-colorings, marking the first progress in 30 years on a key variant involving different forbidden distances and significantly expanding the known solution range.</p> <p style="display: flex; gap: 0.15em; align-items: center;"> <span style="color: var(--global-text-color); font-size: 1.2em; display: inline-flex; align-items: center; padding: 0.2em;"> <i class="fas fa-university"></i> ICML25 </span> <a class="btn btn-sm z-depth-0" href="https://arxiv.org/abs/2501.18527" target="_blank" rel="noopener noreferrer" style="color: var(--global-theme-color); text-decoration: none; font-size: 1.2em; display: inline-flex; align-items: center; padding: 0.2em;"> <i class="fas fa-file-alt"></i> PDF </a> <a class="btn btn-sm z-depth-0" href="https://github.com/ZIB-IOL/neural-discovery-icml25" target="_blank" rel="noopener noreferrer" style="color: var(--global-theme-color); text-decoration: none; font-size: 1.2em; display: inline-flex; align-items: center; padding: 0.2em;"> <i class="fas fa-code"></i> Code </a> </p> </d-title> <d-byline style="margin-top: 0.2em;"></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#the-chromatic-number-of-the-plane">The chromatic number of the plane</a></div> <div><a href="#a-new-approach-neural-networks-for-mathematical-discovery">A New Approach: Neural Networks for Mathematical Discovery</a></div> <ul> <li><a href="#modeling-the-coloring-problem">Modeling the Coloring Problem</a></li> <li><a href="#guiding-mathematical-intuition">Guiding Mathematical Intuition</a></li> </ul> <div><a href="#coloring-variants-and-recent-progress">Coloring Variants and Recent Progress</a></div> <div><a href="#almost-coloring-the-plane">Almost Coloring the Plane</a></div> <div><a href="#conclusion-and-outlook">Conclusion and Outlook</a></div> </nav> </d-contents> <h2 id="the-chromatic-number-of-the-plane">The chromatic number of the plane</h2> <p>The Hadwiger-Nelson (HN) problem, first posed in 1950, asks a fundamental question at the intersection of geometry and combinatorics: What is the minimum number of colors needed to color the entire Euclidean plane $\mathbb{R}^2$ such that no two points at unit distance share the same color? This minimum number is referred to as the <em>chromatic number of the plane</em>, denoted $\chi(\mathbb{R}^2)$, as it is the chromatic number of the infinite graph with vertex set $\mathbb{R}^2$ and edges connecting pairs of points at unit distance. What makes this seemingly simple setup challenging is the combination between discrete choices (the colors) with continuous geometry (the distance on the plane).</p> <p>Despite over 70 years of research, the exact value of $\chi(\mathbb{R}^2)$ remains unknown. It is currently bounded between 5 and 7, i.e., $5 \le \chi(\mathbb{R}^2) \le 7$. Establishing lower bounds typically involves constructing finite <em>unit-distance graphs</em>—graphs with a high chromatic number, that is, graphs that can be embedded in the plane in a way such that edges connect points exactly one unit apart. Notable examples include the Moser spindle, which requires 4 colors <d-cite key="moser1961solution"></d-cite>, and a large graph discovered by Aubrey de Grey in 2018, proving $\chi(\mathbb{R}^2) \ge 5$ <d-cite key="DeGrey2018ChromaticNumber"></d-cite>. Conversely, upper bounds are demonstrated by providing explicit coloring patterns for the entire plane. The best-known upper bound of 7 is realized by many different constructions, the most well known of which is a coloring based on a hexagonal tiling of the plane <d-cite key="soifer2009mathematical"></d-cite>. Closing the gap between 5 and 7 remains a significant open problem in mathematics.</p> <p>In this work, we focus on improving the upper bounds. We explore how neural networks (NNs) can serve as a tool for mathematical discovery, guiding our intuition towards potential new constructions for this problem and its variants.</p> <h2 id="a-new-approach-neural-networks-for-mathematical-discovery">A New Approach: Neural Networks for Mathematical Discovery</h2> <h3 id="modeling-the-coloring-problem">Modeling the Coloring Problem</h3> <p>We introduce a new approach to generate colorings which avoid unit distance conflicts using NNs. The core idea is to reframe the combinatorial problem as a continuous optimization task that NNs are well-suited to handle. Instead of assigning a fixed color (a discrete choice) to each point, we relax the constraint by defining a probabilistic coloring. We train the NN to output a <em>probability distribution</em> over the available colors for any given point $(x, y) \in \mathbb{R}^2$.</p> <p>Let $p_\theta: \mathbb{R}^2 \to \Delta_c$ be an NN parameterized by $\theta$, where $c$ is the number of colors and $\Delta_c$ is the $c$-dimensional probability simplex. For an input point $x \in \mathbb{R}^2$, the output $p_\theta(x) = (p_1, …, p_c)$ represents the probability distribution over the colors that point $x$ can attain.</p> <p>We then train the network in an <em>unsupervised</em> way, that is, without labeled data. Instead, we define a way to measure how “bad” a probabilistic coloring is according to the problem’s constraints. The resulting loss function is differentiable thanks to the probabilistic nature of the relaxed problem, allowing us to measure the violation of the unit distance constraint within a finite region $[-R,R]^2$. For the standard Hadwiger-Nelson problem, the loss function calculates the <em>expected</em> probability that two points $x, y$ exactly one unit apart <em>do</em> share the same color when said colors are sampled from their respective distributions $p_\theta(x)$ and $p_\theta(y)$:</p> \[\mathcal{L}(\theta) = \int_{[-R, R]^2} \int_{\partial B_1(x)} p_\theta(x)^T p_\theta(y) \; \mathrm{d}y \; \mathrm{d}x,\] <p>where $\partial B_1(x)$ is the unit circle around $x$.</p> <p>In practice, we approximate this loss and its gradient using Monte Carlo sampling. At each training step, we sample a batch of $n$ center points $x_i \sim \mathcal{U}([-R, R]^2)$ and for each $x_i$, we sample $m$ points $y_{ij} \sim \mathcal{U}(\partial B_1(x_i))$ from the unit circle around it. The loss for the batch is then approximated by the average conflict probability:</p> \[\mathcal{L}(\theta) \approx \frac{1}{nm} \sum_{i=1}^{n} \sum_{j=1}^{m} p_\theta(x_i)^T p_\theta(y_{ij}).\] <p>The entire setup is differentiable with respect to the network parameters $\theta$, allowing us to use standard gradient descent algorithms (like Adam <d-cite key="kingma2014adam"></d-cite>) to iteratively adjust $\theta$ and minimize the loss. As the loss decreases, the network learns to assign probabilities such that points at unit distance are less likely to receive the same color.</p> <p>The following animation illustrates this learning process, showing how the initially random probabilistic coloring evolves over training iterations to form a structured pattern that minimizes conflicts. We depict $\text{argmax}\ p_\theta(x)$, i.e., for each pixel we choose the color which currently has the highest probability.</p> <div class="figure-container"> <img src="/assets/img/blog_img/neural_discovery_mathematics/training_evolution.gif" alt="Animation illustrating the NN's learning process" style="max-width: 100%;" class="zoomable" data-zoomable=""> <div class="figure-caption">Animation illustrating the NN's learning process. Watch as the initially random probabilistic coloring evolves over training iterations, guided by the loss function, eventually forming a structured pattern that minimizes conflicts.</div> </div> <h3 id="guiding-mathematical-intuition">Guiding Mathematical Intuition</h3> <p>The NN acts as a powerful function approximator, capable of learning complex spatial patterns directly from the geometric constraints, largely without strong built-in assumptions about symmetry or periodicity. Crucially, we use this approach not to find fully automated proofs, but as a tool to guide mathematical intuition <d-cite key="davies2021advancing"></d-cite>. When the loss converges to near-zero, the resulting coloring often reveals highly structured patterns. While not clearly visible from the animation, the network mostly assigns very high probabilities to one color and near-zero probabilities for the others, except for the transitions between the regions. The patterns provide valuable insights that can inspire the development of formal mathematical constructions, which must then be rigorously analyzed and verified. The figure below shows an example of this process, comparing a probabilistic pattern discovered by the NN with the resulting formal construction derived from it.</p> <div class="figure-container"> <img src="/assets/img/blog_img/neural_discovery_mathematics/complete_nn_comparison_compact.jpeg" alt="NN output inspiring a formal 6-coloring" style="max-width: 100%;" class="zoomable" data-zoomable=""> <div class="figure-caption">Left: A probabilistic coloring pattern suggested by the NN after training to minimize conflicts for a $(1,1,1,1,1,d)$ variant (defined later). Right: The formalized mathematical construction derived from the NN's output, published in a specialized venue <d-cite key="2024_SixcoloringsExpansion"></d-cite>. This coloring is valid for a specific range of $d$.</div> </div> <h2 id="coloring-variants-and-recent-progress">Coloring Variants and Recent Progress</h2> <p>While finding a 6-coloring for the original problem remains difficult (our NNs consistently find patterns needing seven colors, aligning with the possibility that $\chi(\mathbb{R}^2)=7$), this NN-based optimization approach proved highly successful on related variants of the problem.</p> <p>A key variant we studied is the $(1,1,1,1,1,d)$ coloring problem: coloring the plane with six colors s.t. pairs of points assigned the first five colors must avoid unit distance, while pairs assigned the sixth color must avoid a <em>different</em> distance $d &gt; 0$. Prior to our work, such colorings were only known to exist for $d \in [\sqrt{2}-1, 1/\sqrt{5}] \approx [0.414, 0.447]$ <d-cite key="hoffman1993almost"></d-cite> <d-cite key="soifer1994infinite"></d-cite>.</p> <p>To tackle this variant, we modify the loss calculation. Instead of penalizing only unit-distance conflicts for all colors, the loss sums the conflict probabilities for each color $k$ with its specific forbidden distance $d_k$. In the sampling approximation, for a batch of points $x_i$, we sample points $y_{ij}^{(k)}$ at distance $d_k$ from $x_i$ for each color $k$, and the loss becomes:</p> \[\mathcal{L}(\theta) \approx \frac{1}{nm} \sum_{i=1}^{n} \sum_{j=1}^{m} \sum_{k=1}^6 p_\theta(x_i)_k \cdot p_\theta(y_{ij}^{(k)})_k\] <p>where $d_1=…=d_5=1$ and $d_6=d$. This loss penalizes pairs $(x_i, y_{ij}^{(k)})$ having the same color $k$ if their distance is the forbidden distance $d_k$.</p> <p>We also explored other variants, such as the $(1,1,1,1,d_1,d_2)$ problem, where two colors have distinct forbidden distances $d_1$ and $d_2$. Our framework easily adapts by modifying the loss calculation to incorporate the specific distance constraints for each color pair. The figure below shows a heatmap of the minimum conflict rate found by the NN for the $(1,1,1,1,d_1,d_2)$ variant across different values of $d_1$ and $d_2$.</p> <div class="figure-container"> <img src="/assets/img/blog_img/neural_discovery_mathematics/heatmap_two_colors_free.png" alt="Heatmap showing minimum conflicts for (1,1,1,1,d1,d2) colorings" style="max-width: 100%;" class="zoomable" data-zoomable=""> <div class="figure-caption">Minimum conflict rate (log scale) found by the NN for the $(1,1,1,1,d_1,d_2)$ variant across different values of $d_1$ and $d_2$. Darker blue indicates lower conflict, suggesting potentially valid colorings exist in those regions. The diagonal corresponds to the $(1,1,1,1,1,d)$ case.</div> </div> <p>Focusing on the $(1,1,1,1,1,d)$ variant, our NN approach, exploring different values of $d$, discovered patterns that guided us to formally construct and verify <strong>two new families of 6-colorings</strong>. These constructions were published in a specialized venue <d-cite key="2024_SixcoloringsExpansion"></d-cite> and are visualized below:</p> <div class="figure-container"> <div style="display: flex; flex-wrap: wrap; justify-content: space-around; align-items: flex-start; margin-bottom: 0.5em;"> <div style="flex: 1; min-width: 300px; margin: 0.5em;"> <img src="/assets/img/blog_img/neural_discovery_mathematics/firstcoloring_complete.jpeg" alt="First new 6-coloring pattern" style="width: 100%;" class="zoomable" data-zoomable=""> </div> <div style="flex: 1; min-width: 300px; margin: 0.5em;"> <img src="/assets/img/blog_img/neural_discovery_mathematics/secondcoloring_complete.jpeg" alt="Second new 6-coloring pattern" style="width: 100%;" class="zoomable" data-zoomable=""> </div> </div> <div class="figure-caption">Our two novel 6-coloring patterns. Pattern 1 (upper image) is valid for $d \in [0.354, 0.553]$. Red avoids distance $d$ (shown for $d=0.45$), others avoid distance 1. Pattern 2 (lower image) is valid for $d \in [0.418, 0.657]$. The dotted circles have unit distance radius, while the dashed circles have radius 0.657 and the dash-dotted circles have radius 0.418. <d-cite key="2024_SixcoloringsExpansion"></d-cite> </div> </div> <p>Together, these NN-inspired constructions <strong>significantly expand the known continuum of realizable distances</strong> $d$ to $[0.354, 0.657]$, representing the first progress on this problem variant in 30 years.</p> <p>Note that the distance $d$ in the last color introduces an additional degree of freedom. While it would be possible to just run our approach for many values of $d$, we realized that we can parametrize whole families of colorings by including the distance $d$ in the input of the NN. Querying then network at $(x, d)$ then yields the probability distribution over the colors at point $x$ if the forbidden distance in the last color is $d$. To visualize the exploration process for the $(1,1,1,1,1,d)$ variant, the following animation shows how the probabilistic 6-coloring found by the NN (left) and its corresponding conflicts (right, where black points indicate violations) change as the forbidden distance $d$ for the sixth color varies:</p> <div class="figure-container"> <img src="/assets/img/blog_img/neural_discovery_mathematics/varying_d.gif" alt="Animation showing coloring and conflicts changing with distance d" style="max-width: 100%;" class="zoomable" data-zoomable=""> <div class="figure-caption">Animation showing how the probabilistic 6-coloring (left) and its corresponding conflicts (right, black points indicate violations) change as the forbidden distance $d$ for the sixth color varies. Note that this visualization is produced by a single NN, which parametrizes a family of colorings.</div> </div> <p>The plot below further summarizes these findings by showing the minimum conflict rate achieved by the NNs across different distances $d$. Lower conflict rates suggest that a valid formal coloring might exist for that $d$. The blue region highlights the significantly expanded range of $d$ where our new constructions provide valid 6-colorings, compared to the previously known range shown in orange.</p> <div class="figure-container"> <img src="/assets/img/blog_img/neural_discovery_mathematics/distance_vs_conflicts.jpeg" alt="Conflict rate vs distance d for (1,1,1,1,1,d) colorings" style="max-width: 100%;" class="zoomable" data-zoomable=""> <div class="figure-caption">Minimum conflict rate found by NNs across different distances $d$ for the $(1,1,1,1,1,d)$ variant. The orange region shows the previously known range of $d$ for valid 6-colorings, while the blue region shows the significantly expanded range enabled by our new NN-discovered constructions.</div> </div> <h2 id="almost-coloring-the-plane">Almost Coloring the Plane</h2> <p>Beyond the distance variant, our framework demonstrated its applicability to other related geometric coloring problems, such as “almost coloring” the plane, arising as a natural extension of the HN problem: what is the minimum fraction of the plane that must be removed such that the remaining points <em>can</em> be colored with $c$ colors without monochromatic unit-distance pairs? We introduced an additional $(c+1)$-th color, corresponding to the removed points, and used a Lagrangian relaxation approach. This involves a modified loss function that penalizes conflicts among the first $c$ colors while also penalizing the use of the $(c+1)$-th “uncolored” color via a Lagrange multiplier $\lambda$.</p> <p>Our numerical experiments for $c=6$ colors recovered patterns resembling known constructions involving intersecting pentagonal rods <d-cite key="pritikin1998all"></d-cite>. The table below summarizes our numerical findings for $c=1$ to $c=6$ colors, showing the approximate minimum density of points that must be removed. These results align well with the best known bounds <d-cite key="pritikin1998all"></d-cite> <d-cite key="parts2020percent"></d-cite>, further validating our approach.</p> <div class="table-container"> <table> <thead> <tr> <th># Colors ($c$)</th> <th>1</th> <th>2</th> <th>3</th> <th>4</th> <th>5</th> <th>6</th> </tr> </thead> <tbody> <tr> <th>Best known density removed</th> <td>77.04%</td> <td>54.13%</td> <td>31.20%</td> <td>8.25%</td> <td>4.01%</td> <td>0.02%</td> </tr> <tr> <th>Our results (approx. density removed)</th> <td>77.07%</td> <td>54.21%</td> <td>31.34%</td> <td>8.29%</td> <td>3.60%</td> <td>0.03%</td> </tr> </tbody> </table> <div class="table-caption"> Minimum density of points that must be removed ("uncolored") to allow a conflict-free coloring of the remaining plane with $c$ colors. Our numerical results align well with known bounds <d-cite key="pritikin1998all"></d-cite> <d-cite key="parts2020percent"></d-cite>. </div> </div> <h2 id="conclusion-and-outlook">Conclusion and Outlook</h2> <p>Our work highlights the potential of NNs as tools for mathematical exploration. By reformulating combinatorial geometry problems, like the Hadwiger-Nelson problem and its variants, as continuous optimization tasks, we can experimentally discover new and potentially interesting patterns. While these networks do not yield formal proofs directly, they effectively explore complex solution spaces and uncover structured patterns that can guide mathematical intuition.</p> <p>The discovery of two novel 6-colorings for the $(1,1,1,1,1,d)$ distance variant, significantly extending the known range of valid distances $d$ and marking the first progress in three decades, serves as a strong testament to this approach’s potential. It demonstrates how gradient-based optimization on continuous relaxations in combination with the powerful approximation capabilities of NNs can uncover combinatorial structures that have long eluded traditional methods, offering a promising new paradigm for tackling longstanding open problems in discrete mathematics and theoretical computer science.</p> <p>If you find this interesting and if this work is helpful for your research, please consider citing our paper:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mundinger2025neural</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Neural Discovery in Mathematics: Do Machines Dream of Colored Planes?}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Mundinger, Konrad and Zimmer, Max and Kiem, Aldo and Spiegel, Christoph and Pokutta, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Forty-second International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://openreview.net/forum?id=7Tp9zjP9At}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/neural_discovery_in_mathematics.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Max Zimmer | <a href="https://maxzimmer.org/legal-notice">[Legal Notice]</a> | <a href="https://maxzimmer.org/privacy-policy">[Privacy Policy]</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-VSXWL40NEP"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-VSXWL40NEP");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> </body> </html>