
@article{davies2021advancing,
  title={Advancing mathematics by guiding human intuition with {AI}},
  author={Davies, Alex and Veli{\v{c}}kovi{\'c}, Petar and Buesing, Lars and Blackwell, Sam and Zheng, Daniel and Toma{\v{s}}ev, Nenad and Tanburn, Richard and Battaglia, Peter and Blundell, Charles and Juh{\'a}sz, Andr{\'a}s and others},
  journal={Nature},
  volume={600},
  number={7887},
  pages={70--74},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{karalias2020erdos,
  title={{Erd\H{o}s} goes neural: an unsupervised learning framework for combinatorial optimization on graphs},
  author={Karalias, Nikolaos and Loukas, Andreas},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6659--6672},
  year={2020}
}

@article{wagner2021constructions,
  title={Constructions in combinatorics via neural networks},
  author={Wagner, Adam Zsolt},
  journal={arXiv preprint arXiv:2104.14516},
  year={2021}
}

@inproceedings{parczyk2023fully,
  title={Fully computer-assisted proofs in extremal combinatorics},
  author={Parczyk, Olaf and Pokutta, Sebastian and Spiegel, Christoph and Szab{\'o}, Tibor},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={10},
  pages={12482--12490},
  year={2023}
}

@article{wang2023scientific,
  title={Scientific discovery in the age of artificial intelligence},
  author={Wang, Hanchen and Fu, Tianfan and Du, Yuanqi and Gao, Wenhao and Huang, Kexin and Liu, Ziming and Chandak, Payal and Liu, Shengchao and Van Katwyk, Peter and Deac, Andreea and others},
  journal={Nature},
  volume={620},
  number={7972},
  pages={47--60},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{krenn2022scientific,
  title={On scientific understanding with artificial intelligence},
  author={Krenn, Mario and Pollice, Robert and Guo, Si Yue and Aldeghi, Matteo and Cervera-Lierta, Alba and Friederich, Pascal and dos Passos Gomes, Gabriel and H{\"a}se, Florian and Jinich, Adrian and Nigam, AkshatKumar and others},
  journal={Nature Reviews Physics},
  volume={4},
  number={12},
  pages={761--769},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{fawzi2022discovering,
  title={Discovering faster matrix multiplication algorithms with reinforcement learning},
  author={Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R Ruiz, Francisco J and Schrittwieser, Julian and Swirszcz, Grzegorz and others},
  journal={Nature},
  volume={610},
  number={7930},
  pages={47--53},
  year={2022},
  publisher={Nature Publishing Group}
}

@article{williamson2023deep,
  title={Is deep learning a useful tool for the pure mathematician?},
  author={Williamson, Geordie},
  journal={arXiv preprint arXiv:2304.12602},
  year={2023}
}

@article{berczi2024note,
  title={A note on small percolating sets on hypercubes via generative {AI}},
  author={B{\'e}rczi, Gergely and Wagner, Adam Zsolt},
  journal={arXiv preprint arXiv:2411.19734},
  year={2024}
}

@article{ghebleh2024reinforcement,
  title={Reinforcement learning for graph theory, {II}. Small {Ramsey} numbers},
  author={Ghebleh, Mohammad and Al-Yakoob, Salem and Kanso, Ali and Stevanovi{\'c}, Dragan},
  journal={arXiv preprint arXiv:2403.20055},
  year={2024}
}

@article{mehrabian2023finding,
  title={Finding increasingly large extremal graphs with {AlphaZero} and tabu search},
  author={Mehrabian, Abbas and Anand, Ankit and Kim, Hyunjik and Sonnerat, Nicolas and Balog, Matej and Comanici, Gheorghe and Berariu, Tudor and Lee, Andrew and Ruoss, Anian and Bulanova, Anna and others},
  journal={arXiv preprint arXiv:2311.03583},
  year={2023}
}

@inproceedings{roucairol2022refutation,
  title={Refutation of spectral graph theory conjectures with {Monte Carlo} search},
  author={Roucairol, Milo and Cazenave, Tristan},
  booktitle={International Computing and Combinatorics Conference},
  pages={162--176},
  year={2022},
  organization={Springer}
}

@article{romera2024mathematical,
  title={Mathematical discoveries from program search with large language models},
  author={Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M Pawan and Dupont, Emilien and Ruiz, Francisco JR and Ellenberg, Jordan S and Wang, Pengming and Fawzi, Omar and others},
  journal={Nature},
  volume={625},
  number={7995},
  pages={468--475},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{davies2021signature,
  title={The signature and cusp geometry of hyperbolic knots},
  author={Davies, Alex and Juh{\'a}sz, Andr{\'a}s and Lackenby, Marc and Tomasev, Nenad},
  journal={arXiv preprint arXiv:2111.15323},
  year={2021}
}

@article{charton2024patternboost,
  title={{PatternBoost}: Constructions in mathematics with a little help from {AI}},
  author={Charton, Fran{\c{c}}ois and Ellenberg, Jordan S and Wagner, Adam Zsolt and Williamson, Geordie},
  journal={arXiv preprint arXiv:2411.00566},
  year={2024}
}

@book{jensen2011graph,
  title={Graph coloring problems},
  author={Jensen, Tommy R and Toft, Bjarne},
  year={2011},
  publisher={John Wiley \& Sons}
}

@book{nash2016open,
  title={Open problems in mathematics},
  author={Nash, John Forbes and Rassias, Michael Th.},
  year={2016},
  publisher={Springer}
}

@book{soifer2009mathematical,
  title={The mathematical coloring book: Mathematics of coloring and the colorful life of its creators},
  author={Soifer, Alexander},
  year={2009},
  publisher={Springer}
}

@article{bruijn1951colour,
  title={A colour problem for infinite graphs and a problem in the theory of relations},
  author={de Bruijn, Nicolaas Govert and Erd\H{o}s, P},
  journal={Indigationes Mathematicae},
  volume={13},
  pages={371--373},
  year={1951}
}

@article{DeGrey2018ChromaticNumber,
  title={The chromatic number of the plane is at least 5},
  author={de Grey, Aubrey D.N.J.},
  journal={Geombinatorics Quarterly},
  volume={XXVIII},
  number={1},
  pages={18-31},
  year={2018},
}

@article{Heule2018ComputingSmallGraphs,
  title={Computing small unit-distance graphs with chromatic number 5},
  author={Heule, Marijn J.H.},
  journal={Geombinatorics Quarterly},
  volume={XXVIII},
  number={1},
  pages={32-50},
  year={2018}
}

@article{Polymath2021ChromaticNumber,
  title={On the chromatic number of circular disks and infinite strips in the plane},
  author={Polymath, D.H.J.},
  journal={Geombinatorics Quarterly},
  volume={XXX},
  number={4},
  pages={190-201},
  year={2021}
}

@article{exoo2020chromatic,
  title={The chromatic number of the plane is at least 5: a new proof},
  author={Exoo, Geoffrey and Ismailescu, Dan},
  journal={Discrete \& Computational Geometry},
  volume={64},
  number={1},
  pages={216--226},
  year={2020},
  publisher={Springer}
}

@article{moser1961solution,
  title={Solution to problem 10},
  author={Moser, Leo and Moser, William},
  journal={Canad. Math. Bull},
  volume={4},
  pages={187--189},
  year={1961}
}

@article{parts2020graph,
  title={Graph minimization, focusing on the example of 5-chromatic unit-distance graphs in the plane},
  author={Parts, Jaan},
  journal={arXiv preprint arXiv:2010.12665},
  year={2020}
}

@article{soifer1992relatives,
  title={Relatives of chromatic number of the plane {I}},
  author={Soifer, Alexander},
  journal={Geombinatorics},
  volume={1},
  number={4},
  pages={13--17},
  year={1992}
}

@article{soifer1992six,
  title={A six-coloring of the plane},
  author={Soifer, Alexander},
  journal={Journal of Combinatorial Theory, Series A},
  volume={61},
  number={2},
  pages={292--294},
  year={1992},
  publisher={Elsevier}
}

@article{Soifer1994SixRealizable,
  title={Six-realizable set {$X_6$}},
  author={Soifer, A.},
  journal={Geombinatorics},
  volume={III},
  number={4},
  pages={140-145},
  year={1994}
}

@article{raiskii1970realization,
  title={Realization of all distances in a decomposition of the space {$\mathbb{R}^n$} into {$n+1$} parts},
  author={Raiskii, Dmitry E},
  journal={Mathematical notes of the Academy of Sciences of the USSR},
  volume={7},
  pages={194--196},
  year={1970},
  publisher={Springer}
}

@article{woodall1973distances,
  title={Distances realized by sets covering the plane},
  author={Woodall, Douglas R},
  journal={Journal of Combinatorial Theory, Series A},
  volume={14},
  number={2},
  pages={187--200},
  year={1973},
  publisher={Elsevier}
}

@article{hoffman1993almost,
  title={Almost chromatic number of the plane},
  author={Hoffman, I and Soifer, A},
  journal={Geombinatorics},
  volume={3},
  number={2},
  pages={38--40},
  year={1993}
}

@article{hoffman1996another,
  title={Another six-coloring of the plane},
  author={Hoffman, Ilya and Soifer, Alexander},
  journal={Discrete Mathematics},
  volume={150},
  number={1-3},
  pages={427--429},
  year={1996},
  publisher={Elsevier}
}

@article{soifer1994infinite,
  title={An infinite class of six-colorings of the plane},
  author={Soifer, Alexander},
  journal={Congressus Numerantium},
  pages={83--86},
  year={1994},
  publisher={UTILITAS MATHEMATICA PUBLISHING INC}
}#

@article{parts2020percent,
  title={What percent of the plane can be properly 5-and 6-colored?},
  author={Parts, Jaan},
  journal={arXiv preprint arXiv:2010.12668},
  year={2020}
}

@article{pritikin1998all,
  title={All unit-distance graphs of order 6197 are 6-colorable},
  author={Pritikin, Dan},
  journal={Journal of Combinatorial Theory, Series B},
  volume={73},
  number={2},
  pages={159--163},
  year={1998},
  publisher={Elsevier}
}



@article{2024_SixcoloringsExpansion,
  author = {Mundinger, Konrad and Pokutta, Sebastian and Spiegel, Christoph and Zimmer, Max},
  journal = {Geombinatorics Quarterly},
  title = {Extending the Continuum of Six-Colorings},
  year = {2024},
  volume = {XXXIV},
  archiveprefix = {arXiv},
  eprint = {2404.05509},
  url = {https://geombina.uccs.edu/past-issues/volume-xxxiv},
  tags = {ai4science}
}

 @article{Berzins_Radler_Volkmann_Sanokowski_Hochreiter_Brandstetter_2024, title={Geometry-Informed Neural Networks}, url={http://arxiv.org/abs/2402.14009}, journal={arXiv preprint arXiv:2402.14009}, DOI={10.48550/arXiv.2402.14009}, note={arXiv:2402.14009 [cs]}, number={arXiv:2402.14009}, publisher={arXiv}, author={Berzins, Arturs and Radler, Andreas and Volkmann, Eric and Sanokowski, Sebastian and Hochreiter, Sepp and Brandstetter, Johannes}, year={2024}, month=oct }

@article{Raissi_Perdikaris_Karniadakis_2019_PINNs, title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations}, volume={378}, ISSN={0021-9991}, DOI={10.1016/j.jcp.2018.10.045}, abstractNote={We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}, journal={Journal of Computational Physics}, author={Raissi, M. and Perdikaris, P. and Karniadakis, G. E.}, year={2019}, month=feb, pages={686–707} }


@article{coulson200215,
  title={A 15-colouring of 3-space omitting distance one},
  author={Coulson, David},
  journal={Discrete mathematics},
  volume={256},
  number={1-2},
  pages={83--90},
  year={2002},
  publisher={Elsevier}
}

@article{Barthe_Guedon_Mendelson_Naor_2005, title={A probabilistic approach to the geometry of the Lpn-ball}, volume={33}, ISSN={0091-1798, 2168-894X}, DOI={10.1214/009117904000000874}, abstractNote={This article investigates, by probabilistic methods, various geometric questions on Bpn, the unit ball of ℓpn. We propose realizations in terms of independent random variables of several distributions on Bpn, including the normalized volume measure. These representations allow us to unify and extend the known results of the sub-independence of coordinate slabs in Bpn. As another application, we compute moments of linear functionals on Bpn, which gives sharp constants in Khinchine’s inequalities on Bpn and determines the ψ2-constant of all directions on Bpn. We also study the extremal values of several Gaussian averages on sections of Bpn (including mean width and ℓ-norm), and derive several monotonicity results as p varies. Applications to balancing vectors in ℓ2 and to covering numbers of polyhedra complete the exposition.}, number={2}, journal={The Annals of Probability}, publisher={Institute of Mathematical Statistics}, author={Barthe, Franck and Guédon, Olivier and Mendelson, Shahar and Naor, Assaf}, year={2005}, month=mar, pages={480–513} }

 @inproceedings{Park_Florence_Straub_Newcombe_Lovegrove_2019, title={DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation}, ISSN={2575-7075}, url={https://ieeexplore.ieee.org/document/8954065}, DOI={10.1109/CVPR.2019.00025}, abstractNote={Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape’s surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape’s boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF’s both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.}, booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, author={Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven}, year={2019}, month=jun, pages={165–174} }

  @inproceedings{Mescheder_Oechsle_Niemeyer_Nowozin_Geiger_2019, title={Occupancy Networks: Learning 3D Reconstruction in Function Space}, ISSN={2575-7075}, url={https://ieeexplore.ieee.org/document/8953655}, DOI={10.1109/CVPR.2019.00459}, abstractNote={With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.}, booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, author={Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas}, year={2019}, month=jun, pages={4455–4465} }


 @article{Chen_Zhang_2019, address={Long Beach, CA, USA}, title={Learning Implicit Fields for Generative Shape Modeling}, rights={https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}, ISBN={9781728132938}, DOI={10.1109/CVPR.2019.00609}, abstractNote={We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit decoder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at https://github.com/czq142857/implicit-decoder.}, journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Chen, Zhiqin and Zhang, Hao}, year={2019}, month=jun, pages={5932–5941} }

 @article{Cuomo_Cola_Giampaolo_Rozza_Raissi_Piccialli_2022, title={Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What’s next}, url={http://arxiv.org/abs/2201.05624}, DOI={10.48550/arXiv.2201.05624}, abstractNote={Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs. This novel methodology has arisen as a multi-task learning framework in which a NN must fit observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla PINN, as well as many other variants, such as physics-constrained neural networks (PCNN), variational hp-VPINN, and conservative PINN (CPINN). The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.}, note={arXiv:2201.05624 [cs]}, number={arXiv:2201.05624}, publisher={arXiv}, author={Cuomo, Salvatore and Cola, Vincenzo Schiano di and Giampaolo, Fabio and Rozza, Gianluigi and Raissi, Maziar and Piccialli, Francesco}, year={2022}, month=jun }


@article{Wang_Sankaran_Wang_Perdikaris_2023, title={An Expert’s Guide to Training Physics-informed Neural Networks}, url={http://arxiv.org/abs/2308.08468}, DOI={10.48550/arXiv.2308.08468}, abstractNote={Physics-informed neural networks (PINNs) have been popularized as a deep learning framework that can seamlessly synthesize observational data and partial differential equation (PDE) constraints. Their practical effectiveness however can be hampered by training pathologies, but also oftentimes by poor choices made by users who lack deep learning expertise. In this paper we present a series of best practices that can significantly improve the training efficiency and overall accuracy of PINNs. We also put forth a series of challenging benchmark problems that highlight some of the most prominent difficulties in training PINNs, and present comprehensive and fully reproducible ablation studies that demonstrate how different architecture choices and training strategies affect the test accuracy of the resulting models. We show that the methods and guiding principles put forth in this study lead to state-of-the-art results and provide strong baselines that future studies should use for comparison purposes. To this end, we also release a highly optimized library in JAX that can be used to reproduce all results reported in this paper, enable future research studies, as well as facilitate easy adaptation to new use-case scenarios.}, note={arXiv:2308.08468 [physics]}, number={arXiv:2308.08468}, publisher={arXiv}, author={Wang, Sifan and Sankaran, Shyam and Wang, Hanwen and Perdikaris, Paris}, year={2023}, month=aug }

 @article{Lu_Jin_Pang_Zhang_Karniadakis_2021, title={Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators}, volume={3}, ISSN={2522-5839}, DOI={10.1038/s42256-021-00302-5}, abstractNote={It is widely known that neural networks (NNs) are universal approximators of continuous functions. However, a less known but powerful result is that a NN with a single hidden layer can accurately approximate any nonlinear continuous operator. This universal approximation theorem of operators is suggestive of the structure and potential of deep neural networks (DNNs) in learning continuous operators or complex systems from streams of scattered data. Here, we thus extend this theorem to DNNs. We design a new network with small generalization error, the deep operator network (DeepONet), which consists of a DNN for encoding the discrete input function space (branch net) and another DNN for encoding the domain of the output functions (trunk net). We demonstrate that DeepONet can learn various explicit operators, such as integrals and fractional Laplacians, as well as implicit operators that represent deterministic and stochastic differential equations. We study different formulations of the input function space and its effect on the generalization error for 16 different diverse applications.}, number={3}, journal={Nature Machine Intelligence}, author={Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em}, year={2021}, month=mar, pages={218–229} }

  @article{Wang_Wang_Perdikaris_2021, title={Learning the solution operator of parametric partial differential equations with physics-informed DeepONets}, volume={7}, DOI={10.1126/sciadv.abi8605}, abstractNote={Partial differential equations (PDEs) play a central role in the mathematical analysis and modeling of complex dynamic processes across all corners of science and engineering. Their solution often requires laborious analytical or computational tools, associated with a cost that is markedly amplified when different scenarios need to be investigated, for example, corresponding to different initial or boundary conditions, different inputs, etc. In this work, we introduce physics-informed DeepONets, a deep learning framework for learning the solution operator of arbitrary PDEs, even in the absence of any paired input-output training data. We illustrate the effectiveness of the proposed framework in rapidly predicting the solution of various types of parametric PDEs up to three orders of magnitude faster compared to conventional PDE solvers, setting a previously unexplored paradigm for modeling and simulation of nonlinear and nonequilibrium processes in science and engineering.}, number={40}, journal={Science Advances}, publisher={American Association for the Advancement of Science}, author={Wang, Sifan and Wang, Hanwen and Perdikaris, Paris}, year={2021}, month=sep, pages={eabi8605} }

@article{Jumper_2021, title={Highly accurate protein structure prediction with AlphaFold}, volume={596}, rights={2021 The Author(s)}, ISSN={1476-4687}, DOI={10.1038/s41586-021-03819-2}, number={7873}, journal={Nature}, publisher={Nature Publishing Group}, author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis}, year={2021}, month=aug, pages={583–589}, language={en} }

 @article{Currier_Moore_Yip_2024, title={Any Two-Coloring of the Plane Contains Monochromatic 3-Term Arithmetic Progressions}, volume={44}, ISSN={1439-6912}, DOI={10.1007/s00493-024-00122-2}, abstractNote={A conjecture of Erdős, Graham, Montgomery, Rothschild, Spencer, and Straus states that, with the exception of equilateral triangles, any two-coloring of the plane will have a monochromatic congruent copy of every three-point configuration. This conjecture is known only for special classes of configurations. In this manuscript, we confirm one of the most natural open cases; that is, every two-coloring of the plane admits a monochromatic congruent copy of any 3-term arithmetic progression.}, number={6}, journal={Combinatorica}, author={Currier, Gabriel and Moore, Kenneth and Yip, Chi Hoi}, year={2024}, month=dec, pages={1367–1380}, language={en} }


@article{pauls2024estimating,
	arxiv = {https://arxiv.org/abs/2406.01076},
	author = {Pauls, Jan and Zimmer, Max and Kelly, Una M. and Schwartz, Martin and Saatchi, Sassan and Ciais, Philippe and Pokutta, Sebastian and Brandt, Martin and Gieseke, Fabian},
	code = {https://github.com/AI4Forest/Global-Canopy-Height-Map},
	journal = {Proceedings of ICML},
	month = {5},
	ptype = {conference},
	title = {{Estimating Canopy Height at Scale}},
	year = {2024}}

 @article{Kovachki_Lanthaler_Stuart_2024, title={Operator Learning: Algorithms and Analysis}, url={http://arxiv.org/abs/2402.15715}, DOI={10.48550/arXiv.2402.15715}, abstractNote={Operator learning refers to the application of ideas from machine learning to approximate (typically nonlinear) operators mapping between Banach spaces of functions. Such operators often arise from physical models expressed in terms of partial differential equations (PDEs). In this context, such approximate operators hold great potential as efficient surrogate models to complement traditional numerical methods in many-query tasks. Being data-driven, they also enable model discovery when a mathematical description in terms of a PDE is not available. This review focuses primarily on neural operators, built on the success of deep neural networks in the approximation of functions defined on finite dimensional Euclidean spaces. Empirically, neural operators have shown success in a variety of applications, but our theoretical understanding remains incomplete. This review article summarizes recent progress and the current state of our theoretical understanding of neural operators, focusing on an approximation theoretic point of view.}, note={arXiv:2402.15715 [cs]}, number={arXiv:2402.15715}, publisher={arXiv}, author={Kovachki, Nikola B. and Lanthaler, Samuel and Stuart, Andrew M.}, year={2024}, month=feb }


@article{Rahaman_Baratin_Arpit_Draxler_Lin_Hamprecht_Bengio_Courville_2019, title={On the Spectral Bias of Neural Networks}, url={http://arxiv.org/abs/1806.08734}, DOI={10.48550/arXiv.1806.08734}, abstractNote={Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with $100%$ accuracy. In this work, we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets emph{easier} with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.}, note={arXiv:1806.08734 [stat]}, number={arXiv:1806.08734}, publisher={arXiv}, author={Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred A. and Bengio, Yoshua and Courville, Aaron}, year={2019}, month=may }


@misc{2024_MundingerZimmerPokutta_Neuralparameterregression,
  archiveprefix = {arXiv},
  eprint = {2403.12764},
  primaryclass = {cs.LG},
  year = {2024},
  author = {Mundinger, Konrad and Zimmer, Max and Pokutta, Sebastian},
  title = {Neural Parameter Regression for Explicit Representations of PDE Solution Operators}
}

@article{raiski70,
	abstract = {Let the sets A1, A2,...,An+1, form a covering of the n-dimensional euclidean space Rn (n>1). Then among these sets can be found a set Ai containing, for every d>0, a pair of points such that the distance between them is equal to d.},
	author = {Raiskii, D.  E. },
	date = {1970/03/01},
	date-added = {2025-01-09 15:07:27 +0100},
	date-modified = {2025-01-09 15:07:27 +0100},
	doi = {10.1007/BF01093113},
	id = {Raiskii1970},
	isbn = {1573-8876},
	journal = {Mathematical notes of the Academy of Sciences of the USSR},
	number = {3},
	pages = {194--196},
	title = {Realization of all distances in a decomposition of the space Rn into n+1 parts},
	url = {https://doi.org/10.1007/BF01093113},
	volume = {7},
	year = {1970},
	bdsk-url-1 = {https://doi.org/10.1007/BF01093113}}

@article{nechushtan2002space,
  title={On the space chromatic number},
  author={Nechushtan, Oren},
  journal={Discrete mathematics},
  volume={256},
  number={1-2},
  pages={499--507},
  year={2002},
  publisher={Elsevier}
}

@article{deGrey2020small,
  author = {de Grey, Aubrey D. N. J.},
  title = {A small 6-chromatic unit-distance graph in {R}3},
  journal = {Geombinatorics Quarterly},
  volume = {XXX},
  number = {1},
  pages = {5--13},
  year = {2020}
}

@inproceedings{aichholzer2019triangles,
  title={Triangles in the colored Euclidean plane},
  author={Aichholzer, Oswin and Perz, Daniel},
  booktitle={35th European Workshop on Computational Geometry (EuroCG 2019)},
  year={2019}
}

@misc{graham_problem58,
    author = {Graham, R.},
    title = {Problem 58: Monochromatic Triangles},
    howpublished = {The Open Problems Project (TOPP)},
    note = {Available at \url{http://cs.smith.edu/~orourke/TOPP/P58.html#Problem.58}},
    year = {2003}
}


@article{sitzmann2020implicit,
  title={Implicit neural representations with periodic activation functions},
  author={Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7462--7473},
  year={2020}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{li2019budgeted,
  title={Budgeted training: Rethinking deep neural network training under resource constraints},
  author={Li, Mengtian and Yumer, Ersin and Ramanan, Deva},
  journal={arXiv preprint arXiv:1905.04753},
  year={2019}
}


@InProceedings{Zimmer2021,
  author    = {Max Zimmer and Christoph Spiegel and Sebastian Pokutta},
  booktitle = {International Conference on Learning Representations},
  title     = {{H}ow {I} {L}earned {T}o {S}top {W}orrying {A}nd {L}ove {R}etraining},
  year      = {2023},
  file      = {:Zimmer2021 - How I Learned to Stop Worrying and Love Retraining.pdf:PDF},
  url       = {https://openreview.net/forum?id=_nF5imFKQI},
}

@article{defazio2023and,
  title={When, why and how much? adaptive learning rate scheduling by refinement},
  author={Defazio, Aaron and Cutkosky, Ashok and Mehta, Harsh and Mishchenko, Konstantin},
  journal={arXiv preprint arXiv:2310.07831},
  year={2023}
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}

